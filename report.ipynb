{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Big Data Analytics – Final Project: Wetter vs. Stromverbrauch (Österreich)\n",
    "\n",
    "**Team:** Gössl Marcel, Schrenk Dominik, Unger Miriam  \n",
    "**Kurs:** Big Data Analytics  \n",
    "**Repository:** https://github.com/Dom4i/BigData-WeatherEnergy\n",
    "\n",
    "**Zeitraum der Analyse:** 01/2015 – 12/2019 (5 Jahre, stündliche Daten)\n",
    "\n",
    "## Ziel & Fragestellung (Story)\n",
    "In diesem Projekt kombinieren wir mehrere öffentliche Datenquellen (u.a. Wetterdaten und Stromverbrauchsdaten), um Einflüsse auf den **stündlichen Stromverbrauch** zu analysieren und daraus ein **einfaches Machine-Learning-Modell mit SparkML** zu trainieren.\n",
    "\n",
    "**Leitfragen:**\n",
    "- Welche Faktoren (Temperatur, Tageszeit, Wochentag, Feiertage, Saisonalität, …) beeinflussen den Stromverbrauch am stärksten?\n",
    "- Wie gut lässt sich der Stromverbrauch aus diesen Features **vorhersagen**?\n",
    "\n",
    "## Vorgehen \n",
    "1. **Data Source Layer:** Datensammlung über API + Webscraping + vorhandene Datasets  \n",
    "2. **Raw Data Analysis:** Datenqualität prüfen, erste statistische Analysen und Visualisierungen  \n",
    "3. **Processing Layer (Spark):** Bereinigung, Feature Engineering, Merging der Datensätze\n",
    "4. **Machine Learning (SparkML):** Training, Evaluation und Interpretation  \n",
    "5. **Data Output Layer:** Storytelling + Visualisierung der wichtigsten Erkenntnisse\n",
    "\n",
    "## Erwartetes Ergebnis\n",
    "- Ein bereinigter und gemergter **Feature-Datensatz (stündlich)**  \n",
    "- Eine nachvollziehbare Analyse, welche Treiber den Verbrauch erklären  \n",
    "- Ein SparkML-Modell mit Metriken (z.B. RMSE/MAE) und interpretierbaren Ergebnissen\n"
   ],
   "id": "23c3b6d5e06a4964"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Abhängigkeiten\n",
    "Die Python-Abhängigkeiten sind im `requirements.txt` dokumentiert.\n",
    "\n",
    "Installation:\n",
    "`pip install -r requirements.txt`\n",
    "\n",
    "## Hier nochmal die wichtigsten Versionen der verwendeten Pakete:\n"
   ],
   "id": "c4f747833fa0640b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-11T09:29:45.019981Z",
     "start_time": "2026-01-11T09:29:45.000403Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sys, platform, importlib\n",
    "\n",
    "def v(pkg):\n",
    "    try:\n",
    "        return importlib.metadata.version(pkg)\n",
    "    except Exception:\n",
    "        return \"n/a\"\n",
    "\n",
    "print(\"Python:\", sys.version.split()[0])\n",
    "print(\"OS:\", platform.platform())\n",
    "for pkg in [\"pyspark\", \"pandas\", \"numpy\", \"matplotlib\", \"requests\", \"influxdb-client\"]:\n",
    "    print(f\"{pkg:15} {v(pkg)}\")\n"
   ],
   "id": "f363909093a9278f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.12.0\n",
      "OS: Windows-11-10.0.26100-SP0\n",
      "pyspark         4.1.1\n",
      "pandas          2.3.3\n",
      "numpy           2.3.5\n",
      "matplotlib      n/a\n",
      "requests        2.32.5\n",
      "influxdb-client 1.49.0\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Big Data Kriterien (5Vs) – Einordnung unseres Projekts\n",
    "\n",
    "Auch wenn unser Datensatz im Vergleich zu industriellen Big-Data-Systemen nicht extrem groß ist, orientiert sich unser Vorgehen an typischen Big-Data-Prozessen.\n",
    "\n",
    "## Volume (Datenmenge)\n",
    "Wir arbeiten mit **stündlichen Daten über 5 Jahre (01/2015–12/2019)**. Durch mehrere Datenquellen (Strom, Wetter, Kalender/Feiertage, Tageslicht) entsteht ein zusammengeführter Feature-Datensatz mit vielen Spalten.\n",
    "\n",
    "## Velocity (Geschwindigkeit)\n",
    "Die Daten werden zwar überwiegend als historische Daten verarbeitet (Batch), aber die verwendete Infrastruktur (API-Calls, NoSQL-Speicherung, Spark-Processing) ist auch für regelmäßig aktualisierte Daten geeignet.\n",
    "\n",
    "## Variety (Vielfalt)\n",
    "Wir nutzen mehrere Datenquellen und Formate:\n",
    "- Stromverbrauch (CSV/Time-Series)\n",
    "- Wetterdaten (API-basiert, teils pro Stadt)\n",
    "- Kalender-/Feiertagsdaten (CSV)\n",
    "- Tageslichtdaten (CSV/abgeleitete Tabellen)\n",
    "\n",
    "## Veracity (Datenqualität)\n",
    "Wir prüfen Datenqualität und Konsistenz (fehlende Werte, Ausreißer, Zeitstempel-Alignment, Einheiten) und dokumentieren Bereinigungsschritte, bevor die Daten gemerged werden.\n",
    "\n",
    "## Value (Mehrwert)\n",
    "Der Mehrwert entsteht durch:\n",
    "- erklärende Analyse (welche Faktoren beeinflussen den Stromverbrauch?)\n",
    "- Feature Engineering (z.B. Lags, Kalender-Features)\n",
    "- ein SparkML-Modell zur **Vorhersage** des stündlichen Stromverbrauchs.\n"
   ],
   "id": "c031b7180f21f22b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Data Source Layer – Datensammlung & Rohdaten\n",
    "\n",
    "In diesem Projekt verwenden wir mehrere Datenquellen. Die Rohdaten werden im Ordner `data/raw/` abgelegt und anschließend schrittweise bereinigt und zusammengeführt.\n",
    "\n",
    "\n",
    "## 1. Stromverbrauch (stündlich, Europa) \n",
    "   - Quelle: Open Power System Data (OPSD) – Time Series Dataset  \n",
    "   - Datenformat: CSV  \n",
    "   - Hinweis: Download erfolgt manuell (kein API/Code notwendig).\n",
    "   - Link: https://data.open-power-system-data.org/time_series/2020-10-06\n"
   ],
   "id": "2925d432367a0f03"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##  2. Wetterdaten (stündlich) – Meteostat (RapidAPI)\n",
    "\n",
    "Wir benötigen stündliche Wetterdaten für Österreich.  \n",
    "Da ein flächendeckender Abruf für ganz Österreich mit einem einzelnen API-Key das Anfragelimit überschreiten würde, verwenden wir als praktikable Näherung die **9 Landeshauptstädte** und holen die Daten stündlich pro Stadt.\n",
    "\n",
    "**Technische Umsetzung (API-Limit):**\n",
    "- Abfrage wird **monatlich** gestartet (2015-01 bis 2019-12)\n",
    "- Innerhalb eines Monats werden Daten zusätzlich in **~30-Tage-Chunks** geladen\n",
    "- Es enststehen also pro Stadt 60 Files (5 Jahre x 12 Monate)\n",
    "- Ergebnisse werden als CSV pro Monat gespeichert:\n",
    "  - `data/raw/Wetterdata/<city>/<city>_YYYY_MM.csv`\n",
    "\n",
    "> Hinweis: Wir dokumentieren den API-Code, führen ihn im Notebook aber standardmäßig nicht aus.\n"
   ],
   "id": "7b4f5b02fb1d4cc7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Meteostat RapidAPI Collection Script (Dokumentation) \n",
    "# Standardmäßig NICHT ausführen, um API Limits zu schonen:\n",
    "RUN_API = False\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# CONFIG\n",
    "cities = {\n",
    "    \"wien\": (48.2082, 16.3738),\n",
    "    \"st_poelten\": (48.2049, 15.6256),\n",
    "    \"linz\": (48.3069, 14.2858),\n",
    "    \"salzburg\": (47.8095, 13.0550),\n",
    "    \"graz\": (47.0707, 15.4395),\n",
    "    \"klagenfurt\": (46.6247, 14.3053),\n",
    "    \"innsbruck\": (47.2692, 11.4041),\n",
    "    \"bregenz\": (47.5031, 9.7471),\n",
    "    \"eisenstadt\": (47.8456, 16.5233),\n",
    "}\n",
    "\n",
    "# Im Projekt wurde pro Stadt separat ausgeführt (wegen API Key Limit / Key-Wechsel)\n",
    "CITY_NAME = \"eisenstadt\"\n",
    "LAT = 47.8456\n",
    "LON = 16.5233\n",
    "\n",
    "START_DATE = \"2015-01-01\"\n",
    "END_DATE = \"2019-12-31\"\n",
    "\n",
    "API_KEY = os.getenv(\"\")  \n",
    "\n",
    "BASE_URL = \"https://meteostat.p.rapidapi.com/point/hourly\"\n",
    "HEADERS = {\n",
    "    \"X-RapidAPI-Key\": API_KEY,\n",
    "    \"X-RapidAPI-Host\": \"meteostat.p.rapidapi.com\"\n",
    "}\n",
    "\n",
    "OUTPUT_DIR = Path(\"data/raw/Wetterdata\") / CITY_NAME\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def month_start_dates(start, end):\n",
    "    dates = []\n",
    "    current = start.replace(day=1)\n",
    "    while current <= end:\n",
    "        dates.append(current)\n",
    "        if current.month == 12:\n",
    "            current = current.replace(year=current.year + 1, month=1)\n",
    "        else:\n",
    "            current = current.replace(month=current.month + 1)\n",
    "    return dates\n",
    "\n",
    "if RUN_API:\n",
    "    start_dt = datetime.fromisoformat(START_DATE)\n",
    "    end_dt = datetime.fromisoformat(END_DATE)\n",
    "\n",
    "    for month_start in month_start_dates(start_dt, end_dt):\n",
    "        # Monatsende bestimmen\n",
    "        if month_start.month == 12:\n",
    "            next_month = month_start.replace(year=month_start.year + 1, month=1)\n",
    "        else:\n",
    "            next_month = month_start.replace(month=month_start.month + 1)\n",
    "\n",
    "        month_end = min(next_month - timedelta(days=1), end_dt)\n",
    "\n",
    "        print(f\"Lade {CITY_NAME} {month_start.strftime('%Y-%m')}\")\n",
    "\n",
    "        all_chunks = []\n",
    "        chunk_start = month_start\n",
    "\n",
    "        while chunk_start <= month_end:\n",
    "            chunk_end = min(chunk_start + timedelta(days=29), month_end)\n",
    "\n",
    "            params = {\n",
    "                \"lat\": LAT,\n",
    "                \"lon\": LON,\n",
    "                \"start\": chunk_start.strftime(\"%Y-%m-%d\"),\n",
    "                \"end\": chunk_end.strftime(\"%Y-%m-%d\")\n",
    "            }\n",
    "\n",
    "            response = requests.get(BASE_URL, headers=HEADERS, params=params)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            data = response.json().get(\"data\", [])\n",
    "            if data:\n",
    "                df_chunk = pd.DataFrame(data)\n",
    "                all_chunks.append(df_chunk)\n",
    "\n",
    "            chunk_start = chunk_end + timedelta(days=1)\n",
    "\n",
    "        if all_chunks:\n",
    "            df_month = pd.concat(all_chunks, ignore_index=True)\n",
    "            df_month[\"time\"] = pd.to_datetime(df_month[\"time\"])\n",
    "\n",
    "            filename = f\"{CITY_NAME}_{month_start.year}_{month_start.month:02d}.csv\"\n",
    "            df_month.to_csv(OUTPUT_DIR / filename, index=False)\n",
    "\n",
    "            print(f\"Gespeichert: {filename} ({len(df_month)} Zeilen)\")\n",
    "        else:\n",
    "            print(f\"Keine Daten für {month_start.strftime('%Y-%m')}\")\n",
    "\n",
    "    print(\"Fertig!\")\n",
    "else:\n",
    "    print(\"RUN_API=False → API Calls werden nicht ausgeführt (nur dokumentiert).\")\n"
   ],
   "id": "6b0b1b4f266ce178"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 3. Webscraping – Feiertage in Österreich (2015–2020)\n",
    "\n",
    "Für Kalender- und Saisonalitäts-Features (z.B. *ist Feiertag?*) benötigen wir eine Liste der österreichischen Feiertage.\n",
    "Da wir die Feiertage nicht aus einer fertigen API übernommen haben, nutzen wir Webscraping.\n",
    "\n",
    "**Quelle:** ferienwiki.at (Jahresübersicht je Jahr)  \n",
    "**Ziel:** Erstellen einer sauberen Feiertags-Tabelle (Datum + Name + Wochentag) als CSV in `data/raw/Feiertage/`.\n",
    "\n"
   ],
   "id": "59e1d1e9f3567ffc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-11T09:57:22.039093Z",
     "start_time": "2026-01-11T09:57:18.332584Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Webscraping: Feiertage Österreich \n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "BASE_URL = \"https://www.ferienwiki.at/feiertage/{}/at\"\n",
    "\n",
    "raw_out = Path(\"data/raw/Feiertage/feiertage_at_2015_2020_raw.csv\")\n",
    "raw_out.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "data = []\n",
    "\n",
    "\n",
    "for year in range(2015, 2021):\n",
    "    print(f\"Lade Daten für {year}...\")\n",
    "    url = BASE_URL.format(year)\n",
    "\n",
    "    response = requests.get(url, timeout=30)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    table = soup.find(\"table\", class_=\"table\")\n",
    "    if table is None:\n",
    "        raise RuntimeError(f\"Keine Tabelle gefunden für Jahr {year} ({url})\")\n",
    "\n",
    "    tbody = table.find(\"tbody\")\n",
    "    if tbody is None:\n",
    "        raise RuntimeError(f\"Kein <tbody> gefunden für Jahr {year} ({url})\")\n",
    "\n",
    "    for row in tbody.find_all(\"tr\"):\n",
    "        cols = row.find_all(\"td\")\n",
    "        if len(cols) != 4:\n",
    "            continue\n",
    "\n",
    "        name = cols[0].get_text(strip=True) or \"Unbekannt\"\n",
    "        date_text = cols[1].get_text(strip=True)\n",
    "        if not date_text:\n",
    "            continue\n",
    "\n",
    "        data.append({\n",
    "            \"year\": year,\n",
    "            \"holiday_name\": name,\n",
    "            \"date_raw\": date_text\n",
    "        })\n",
    "\n",
    "df_raw = pd.DataFrame(data)\n",
    "print(\"Rows:\", len(df_raw))\n",
    "df_raw.to_csv(raw_out, index=False)\n",
    "print(\"Gespeichert:\", raw_out)\n",
    "\n"
   ],
   "id": "6627d9465833fa4c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lade Daten für 2015...\n",
      "Lade Daten für 2016...\n",
      "Lade Daten für 2017...\n",
      "Lade Daten für 2018...\n",
      "Lade Daten für 2019...\n",
      "Lade Daten für 2020...\n",
      "Rows: 81\n",
      "Gespeichert: data\\raw\\Feiertage\\feiertage_at_2015_2020_raw.csv\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 4. Tageslicht (Sunrise/Sunset) als Feature\n",
    "\n",
    "Für Stromverbrauch ist Tageslicht bzw. die Länge des Tages oft ein relevanter Einflussfaktor (Saisonalität).\n",
    "Statt nur “Monat” zu verwenden, erzeugen wir ein genaueres Feature basierend auf **Sonnenaufgang/-untergang**.\n",
    "\n",
    "## Vorgehen\n",
    "1. **Geocoding der Landeshauptstädte** (Latitude/Longitude) über Nominatim (OpenStreetMap)  \n",
    "2. **Berechnung von Sonnenaufgang und Sonnenuntergang** pro Tag und Stadt mit `astral`  \n",
    "3. **Durchschnitt über Österreich** als Mittelwert über die Landeshauptstädte  \n",
    "4. Speicherung als tägliche CSV in `data/raw/Tageslicht/`\n",
    "\n"
   ],
   "id": "10f630b142f8e03c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-11T10:06:27.265770Z",
     "start_time": "2026-01-11T10:06:18.381091Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from geopy.geocoders import Nominatim\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "CITIES = [\n",
    "    \"Bregenz\",\n",
    "    \"Innsbruck\",\n",
    "    \"Salzburg\",\n",
    "    \"Linz\",\n",
    "    \"Wien\",\n",
    "    \"Graz\",\n",
    "    \"Klagenfurt\"\n",
    "]\n",
    "\n",
    "out_json = Path(\"data/util/austria_places_latlon.json\")\n",
    "out_json.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "geolocator = Nominatim(user_agent=\"austria-daylight-analysis\")\n",
    "\n",
    "def geocode_city(city: str):\n",
    "    location = geolocator.geocode(f\"{city}, Austria\")\n",
    "    if location is None:\n",
    "        raise ValueError(f\"Ort nicht gefunden: {city}\")\n",
    "    return {\"latitude\": round(location.latitude, 6), \"longitude\": round(location.longitude, 6)}\n",
    "\n",
    "places = {}\n",
    "for city in CITIES:\n",
    "    print(f\"Geocoding {city} ...\")\n",
    "    places[city] = geocode_city(city)\n",
    "    time.sleep(1)  # Rate-Limit beachten\n",
    "\n",
    "with open(out_json, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(places, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"Koordinaten gespeichert:\", out_json)\n"
   ],
   "id": "260f6c888c993a46",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geocoding Bregenz ...\n",
      "Geocoding Innsbruck ...\n",
      "Geocoding Salzburg ...\n",
      "Geocoding Linz ...\n",
      "Geocoding Wien ...\n",
      "Geocoding Graz ...\n",
      "Geocoding Klagenfurt ...\n",
      "Koordinaten gespeichert: data\\util\\austria_places_latlon.json\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-11T10:07:04.918844Z",
     "start_time": "2026-01-11T10:07:04.417076Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datetime import date, datetime, timedelta\n",
    "from zoneinfo import ZoneInfo\n",
    "from astral import LocationInfo\n",
    "from astral.sun import sun\n",
    "import csv\n",
    "import json\n",
    "import statistics\n",
    "from pathlib import Path\n",
    "\n",
    "TZ = ZoneInfo(\"Europe/Vienna\")\n",
    "\n",
    "places_file = Path(\"data/util/austria_places_latlon.json\")\n",
    "with open(places_file, encoding=\"utf-8\") as f:\n",
    "    PLACES = json.load(f)\n",
    "\n",
    "def seconds_since_midnight(dt: datetime) -> int:\n",
    "    return dt.hour * 3600 + dt.minute * 60 + dt.second\n",
    "\n",
    "def hhmmss_from_seconds(sec: int) -> str:\n",
    "    sec = int(round(sec)) % 86400\n",
    "    h = sec // 3600\n",
    "    m = (sec % 3600) // 60\n",
    "    s = sec % 60\n",
    "    return f\"{h:02d}:{m:02d}:{s:02d}\"\n",
    "\n",
    "def sunrise_sunset_for_place(lat: float, lon: float, d: date):\n",
    "    loc = LocationInfo(\"x\", \"AT\", \"Europe/Vienna\", lat, lon)\n",
    "    s = sun(loc.observer, date=d, tzinfo=TZ)\n",
    "    return s[\"sunrise\"], s[\"sunset\"]\n",
    "\n",
    "def austria_avg_sunrise_sunset(d: date):\n",
    "    sunrise_secs, sunset_secs = [], []\n",
    "    for coords in PLACES.values():\n",
    "        sr, ss = sunrise_sunset_for_place(coords[\"latitude\"], coords[\"longitude\"], d)\n",
    "        sunrise_secs.append(seconds_since_midnight(sr))\n",
    "        sunset_secs.append(seconds_since_midnight(ss))\n",
    "    return int(round(statistics.mean(sunrise_secs))), int(round(statistics.mean(sunset_secs)))\n",
    "\n",
    "def daterange(start: date, end: date):\n",
    "    cur = start\n",
    "    while cur <= end:\n",
    "        yield cur\n",
    "        cur += timedelta(days=1)\n",
    "\n",
    "START_DATE = \"2015-01-01\"\n",
    "END_DATE = \"2019-12-31\"\n",
    "start = date.fromisoformat(START_DATE)\n",
    "end = date.fromisoformat(END_DATE)\n",
    "\n",
    "out_csv = Path(\"data/raw/Tageslicht/austria_sunrise_sunset_avg_last5y_daily.csv\")\n",
    "out_csv.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(out_csv, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    w = csv.writer(f)\n",
    "    w.writerow([\"date\", \"sunrise_avg\", \"sunset_avg\", \"sunrise_avg_seconds\", \"sunset_avg_seconds\"])\n",
    "    for d in daterange(start, end):\n",
    "        sr_sec, ss_sec = austria_avg_sunrise_sunset(d)\n",
    "        w.writerow([d.isoformat(), hhmmss_from_seconds(sr_sec), hhmmss_from_seconds(ss_sec), sr_sec, ss_sec])\n",
    "\n",
    "print(\"Tageslicht CSV gespeichert:\", out_csv)\n"
   ],
   "id": "ebb74f41c780e8b4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tageslicht CSV gespeichert: data\\raw\\Tageslicht\\austria_sunrise_sunset_avg_last5y_daily.csv\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Kurzer Rohdaten-Check (alle 4 Quellen)\n",
    "\n",
    "Wir prüfen, ob die Rohdaten-Dateien vorhanden sind und zeigen pro Quelle ein paar Beispielzeilen.\n"
   ],
   "id": "3ac2aea4725f4ce6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-11T10:09:46.387333Z",
     "start_time": "2026-01-11T10:09:46.317750Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# 1) Strom (OPSD) – nur ein paar Zeilen laden (große Datei)\n",
    "energy_path = Path(\"data/raw/Strom/time_series_60min_singleindex.csv\")\n",
    "print(\"1) Energy exists:\", energy_path.exists(), \"|\", energy_path)\n",
    "\n",
    "if energy_path.exists():\n",
    "    df_energy_preview = pd.read_csv(energy_path, nrows=5)\n",
    "    display(df_energy_preview)\n",
    "\n",
    "# 2) Wetter (Meteostat) – irgendeine vorhandene Monatsdatei automatisch finden\n",
    "weather_root = Path(\"data/raw/Wetterdata\")\n",
    "weather_files = sorted(glob.glob(str(weather_root / \"*\" / \"*.csv\")))\n",
    "print(\"\\n2) Weather folder exists:\", weather_root.exists(), \"| files found:\", len(weather_files))\n",
    "\n",
    "if weather_files:\n",
    "    sample_weather = Path(weather_files[0])\n",
    "    print(\"Sample weather file:\", sample_weather)\n",
    "    df_weather_preview = pd.read_csv(sample_weather, nrows=5)\n",
    "    display(df_weather_preview)\n",
    "\n",
    "# 3) Feiertage – clean CSV\n",
    "holidays_path = Path(\"data/raw/Feiertage/feiertage_at_2015_2020_clean.csv\")\n",
    "print(\"\\n3) Holidays exists:\", holidays_path.exists(), \"|\", holidays_path)\n",
    "\n",
    "if holidays_path.exists():\n",
    "    df_holidays_preview = pd.read_csv(holidays_path, nrows=10)\n",
    "    display(df_holidays_preview)\n",
    "\n",
    "# 4) Tageslicht – daily CSV\n",
    "daylight_path = Path(\"data/raw/Tageslicht/austria_sunrise_sunset_avg_last5y_daily.csv\")\n",
    "print(\"\\n4) Daylight exists:\", daylight_path.exists(), \"|\", daylight_path)\n",
    "\n",
    "if daylight_path.exists():\n",
    "    df_daylight_preview = pd.read_csv(daylight_path, nrows=10)\n",
    "    display(df_daylight_preview)\n"
   ],
   "id": "614d7ffdeef390f0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) Energy exists: True | data\\raw\\Strom\\time_series_60min_singleindex.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "          utc_timestamp        cet_cest_timestamp  \\\n",
       "0  2014-12-31T23:00:00Z  2015-01-01T00:00:00+0100   \n",
       "1  2015-01-01T00:00:00Z  2015-01-01T01:00:00+0100   \n",
       "2  2015-01-01T01:00:00Z  2015-01-01T02:00:00+0100   \n",
       "3  2015-01-01T02:00:00Z  2015-01-01T03:00:00+0100   \n",
       "4  2015-01-01T03:00:00Z  2015-01-01T04:00:00+0100   \n",
       "\n",
       "   AT_load_actual_entsoe_transparency  AT_load_forecast_entsoe_transparency  \\\n",
       "0                                 NaN                                   NaN   \n",
       "1                              5946.0                                6701.0   \n",
       "2                              5726.0                                6593.0   \n",
       "3                              5347.0                                6482.0   \n",
       "4                              5249.0                                6454.0   \n",
       "\n",
       "   AT_price_day_ahead  AT_solar_generation_actual  \\\n",
       "0                 NaN                         NaN   \n",
       "1                35.0                         NaN   \n",
       "2                45.0                         NaN   \n",
       "3                41.0                         NaN   \n",
       "4                38.0                         NaN   \n",
       "\n",
       "   AT_wind_onshore_generation_actual  BE_load_actual_entsoe_transparency  \\\n",
       "0                                NaN                                 NaN   \n",
       "1                               69.0                              9484.0   \n",
       "2                               64.0                              9152.0   \n",
       "3                               65.0                              8799.0   \n",
       "4                               64.0                              8567.0   \n",
       "\n",
       "   BE_load_forecast_entsoe_transparency  BE_solar_generation_actual  ...  \\\n",
       "0                                   NaN                         NaN  ...   \n",
       "1                                9897.0                         NaN  ...   \n",
       "2                                9521.0                         NaN  ...   \n",
       "3                                9135.0                         NaN  ...   \n",
       "4                                8909.0                         NaN  ...   \n",
       "\n",
       "   SI_load_actual_entsoe_transparency  SI_load_forecast_entsoe_transparency  \\\n",
       "0                                 NaN                                   NaN   \n",
       "1                                 NaN                                   NaN   \n",
       "2                             1045.47                                 816.0   \n",
       "3                             1004.79                                 805.0   \n",
       "4                              983.79                                 803.0   \n",
       "\n",
       "   SI_solar_generation_actual  SI_wind_onshore_generation_actual  \\\n",
       "0                         NaN                                NaN   \n",
       "1                         NaN                                NaN   \n",
       "2                         NaN                               1.17   \n",
       "3                         NaN                               1.04   \n",
       "4                         NaN                               1.61   \n",
       "\n",
       "   SK_load_actual_entsoe_transparency  SK_load_forecast_entsoe_transparency  \\\n",
       "0                                 NaN                                   NaN   \n",
       "1                                 NaN                                   NaN   \n",
       "2                              2728.0                                2860.0   \n",
       "3                              2626.0                                2810.0   \n",
       "4                              2618.0                                2780.0   \n",
       "\n",
       "   SK_solar_generation_actual  SK_wind_onshore_generation_actual  \\\n",
       "0                         NaN                                NaN   \n",
       "1                         NaN                                NaN   \n",
       "2                         3.8                                NaN   \n",
       "3                         3.8                                NaN   \n",
       "4                         3.8                                NaN   \n",
       "\n",
       "   UA_load_actual_entsoe_transparency  UA_load_forecast_entsoe_transparency  \n",
       "0                                 NaN                                   NaN  \n",
       "1                                 NaN                                   NaN  \n",
       "2                                 NaN                                   NaN  \n",
       "3                                 NaN                                   NaN  \n",
       "4                                 NaN                                   NaN  \n",
       "\n",
       "[5 rows x 300 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>utc_timestamp</th>\n",
       "      <th>cet_cest_timestamp</th>\n",
       "      <th>AT_load_actual_entsoe_transparency</th>\n",
       "      <th>AT_load_forecast_entsoe_transparency</th>\n",
       "      <th>AT_price_day_ahead</th>\n",
       "      <th>AT_solar_generation_actual</th>\n",
       "      <th>AT_wind_onshore_generation_actual</th>\n",
       "      <th>BE_load_actual_entsoe_transparency</th>\n",
       "      <th>BE_load_forecast_entsoe_transparency</th>\n",
       "      <th>BE_solar_generation_actual</th>\n",
       "      <th>...</th>\n",
       "      <th>SI_load_actual_entsoe_transparency</th>\n",
       "      <th>SI_load_forecast_entsoe_transparency</th>\n",
       "      <th>SI_solar_generation_actual</th>\n",
       "      <th>SI_wind_onshore_generation_actual</th>\n",
       "      <th>SK_load_actual_entsoe_transparency</th>\n",
       "      <th>SK_load_forecast_entsoe_transparency</th>\n",
       "      <th>SK_solar_generation_actual</th>\n",
       "      <th>SK_wind_onshore_generation_actual</th>\n",
       "      <th>UA_load_actual_entsoe_transparency</th>\n",
       "      <th>UA_load_forecast_entsoe_transparency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2014-12-31T23:00:00Z</td>\n",
       "      <td>2015-01-01T00:00:00+0100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-01-01T00:00:00Z</td>\n",
       "      <td>2015-01-01T01:00:00+0100</td>\n",
       "      <td>5946.0</td>\n",
       "      <td>6701.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>69.0</td>\n",
       "      <td>9484.0</td>\n",
       "      <td>9897.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-01-01T01:00:00Z</td>\n",
       "      <td>2015-01-01T02:00:00+0100</td>\n",
       "      <td>5726.0</td>\n",
       "      <td>6593.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>64.0</td>\n",
       "      <td>9152.0</td>\n",
       "      <td>9521.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1045.47</td>\n",
       "      <td>816.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.17</td>\n",
       "      <td>2728.0</td>\n",
       "      <td>2860.0</td>\n",
       "      <td>3.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-01-01T02:00:00Z</td>\n",
       "      <td>2015-01-01T03:00:00+0100</td>\n",
       "      <td>5347.0</td>\n",
       "      <td>6482.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8799.0</td>\n",
       "      <td>9135.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1004.79</td>\n",
       "      <td>805.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2626.0</td>\n",
       "      <td>2810.0</td>\n",
       "      <td>3.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-01-01T03:00:00Z</td>\n",
       "      <td>2015-01-01T04:00:00+0100</td>\n",
       "      <td>5249.0</td>\n",
       "      <td>6454.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>64.0</td>\n",
       "      <td>8567.0</td>\n",
       "      <td>8909.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>983.79</td>\n",
       "      <td>803.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.61</td>\n",
       "      <td>2618.0</td>\n",
       "      <td>2780.0</td>\n",
       "      <td>3.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 300 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2) Weather folder exists: True | files found: 540\n",
      "Sample weather file: data\\raw\\Wetterdata\\Linz\\Linz_2015_01.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                  time  temp  dwpt  rhum  prcp  snow   wdir  wspd  wpgt  \\\n",
       "0  2015-01-01 00:00:00  -2.2  -3.0  94.0   0.1   NaN    NaN   7.2   NaN   \n",
       "1  2015-01-01 01:00:00   NaN   NaN   NaN   0.0   NaN  310.0   3.6   NaN   \n",
       "2  2015-01-01 02:00:00   0.0  -2.1  86.0   0.1   NaN  260.0  11.2   NaN   \n",
       "3  2015-01-01 03:00:00  -0.8  -1.9  92.0   0.0   NaN  260.0  10.8   NaN   \n",
       "4  2015-01-01 04:00:00  -1.0  -2.0  93.0   0.0   NaN  260.0  13.0   NaN   \n",
       "\n",
       "     pres  tsun  coco  \n",
       "0  1037.9   NaN   NaN  \n",
       "1     NaN   NaN   NaN  \n",
       "2     NaN   NaN   NaN  \n",
       "3  1037.8   NaN   NaN  \n",
       "4     NaN   NaN   NaN  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>temp</th>\n",
       "      <th>dwpt</th>\n",
       "      <th>rhum</th>\n",
       "      <th>prcp</th>\n",
       "      <th>snow</th>\n",
       "      <th>wdir</th>\n",
       "      <th>wspd</th>\n",
       "      <th>wpgt</th>\n",
       "      <th>pres</th>\n",
       "      <th>tsun</th>\n",
       "      <th>coco</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-01-01 00:00:00</td>\n",
       "      <td>-2.2</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1037.9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-01-01 01:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>310.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-01-01 02:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.1</td>\n",
       "      <td>86.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>260.0</td>\n",
       "      <td>11.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-01-01 03:00:00</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>-1.9</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>260.0</td>\n",
       "      <td>10.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1037.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-01-01 04:00:00</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>260.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3) Holidays exists: True | data\\raw\\Feiertage\\feiertage_at_2015_2020_clean.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "   year  month  day        date     weekday         holiday_name\n",
       "0  2015      1    1  2015-01-01  Donnerstag              Neujahr\n",
       "1  2015      1    6  2015-01-06    Dienstag  Heilige Drei Könige\n",
       "2  2015      4    6  2015-04-06      Montag          Ostermontag\n",
       "3  2015      5    1  2015-05-01     Freitag       Staatsfeiertag\n",
       "4  2015      5   14  2015-05-14  Donnerstag  Christi Himmelfahrt\n",
       "5  2015      5   25  2015-05-25      Montag        Pfingstmontag\n",
       "6  2015      6    4  2015-06-04  Donnerstag         Fronleichnam\n",
       "7  2015      8   15  2015-08-15     Samstag    Mariä Himmelfahrt\n",
       "8  2015     10   26  2015-10-26      Montag     Nationalfeiertag\n",
       "9  2015     11    1  2015-11-01     Sonntag        Allerheiligen"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>date</th>\n",
       "      <th>weekday</th>\n",
       "      <th>holiday_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>Donnerstag</td>\n",
       "      <td>Neujahr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2015-01-06</td>\n",
       "      <td>Dienstag</td>\n",
       "      <td>Heilige Drei Könige</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>2015-04-06</td>\n",
       "      <td>Montag</td>\n",
       "      <td>Ostermontag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2015-05-01</td>\n",
       "      <td>Freitag</td>\n",
       "      <td>Staatsfeiertag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015</td>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "      <td>2015-05-14</td>\n",
       "      <td>Donnerstag</td>\n",
       "      <td>Christi Himmelfahrt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2015</td>\n",
       "      <td>5</td>\n",
       "      <td>25</td>\n",
       "      <td>2015-05-25</td>\n",
       "      <td>Montag</td>\n",
       "      <td>Pfingstmontag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2015</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>2015-06-04</td>\n",
       "      <td>Donnerstag</td>\n",
       "      <td>Fronleichnam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2015</td>\n",
       "      <td>8</td>\n",
       "      <td>15</td>\n",
       "      <td>2015-08-15</td>\n",
       "      <td>Samstag</td>\n",
       "      <td>Mariä Himmelfahrt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2015</td>\n",
       "      <td>10</td>\n",
       "      <td>26</td>\n",
       "      <td>2015-10-26</td>\n",
       "      <td>Montag</td>\n",
       "      <td>Nationalfeiertag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2015</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>2015-11-01</td>\n",
       "      <td>Sonntag</td>\n",
       "      <td>Allerheiligen</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4) Daylight exists: True | data\\raw\\Tageslicht\\austria_sunrise_sunset_avg_last5y_daily.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "         date sunrise_avg sunset_avg  sunrise_avg_seconds  sunset_avg_seconds\n",
       "0  2015-01-01    07:54:15   16:24:38                28455               59078\n",
       "1  2015-01-02    07:54:15   16:25:35                28455               59135\n",
       "2  2015-01-03    07:54:11   16:26:35                28451               59195\n",
       "3  2015-01-04    07:54:05   16:27:38                28445               59258\n",
       "4  2015-01-05    07:53:55   16:28:42                28435               59322\n",
       "5  2015-01-06    07:53:44   16:29:48                28424               59388\n",
       "6  2015-01-07    07:53:28   16:30:56                28408               59456\n",
       "7  2015-01-08    07:53:11   16:32:06                28391               59526\n",
       "8  2015-01-09    07:52:50   16:33:18                28370               59598\n",
       "9  2015-01-10    07:52:27   16:34:31                28347               59671"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>sunrise_avg</th>\n",
       "      <th>sunset_avg</th>\n",
       "      <th>sunrise_avg_seconds</th>\n",
       "      <th>sunset_avg_seconds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>07:54:15</td>\n",
       "      <td>16:24:38</td>\n",
       "      <td>28455</td>\n",
       "      <td>59078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-01-02</td>\n",
       "      <td>07:54:15</td>\n",
       "      <td>16:25:35</td>\n",
       "      <td>28455</td>\n",
       "      <td>59135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-01-03</td>\n",
       "      <td>07:54:11</td>\n",
       "      <td>16:26:35</td>\n",
       "      <td>28451</td>\n",
       "      <td>59195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-01-04</td>\n",
       "      <td>07:54:05</td>\n",
       "      <td>16:27:38</td>\n",
       "      <td>28445</td>\n",
       "      <td>59258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-01-05</td>\n",
       "      <td>07:53:55</td>\n",
       "      <td>16:28:42</td>\n",
       "      <td>28435</td>\n",
       "      <td>59322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2015-01-06</td>\n",
       "      <td>07:53:44</td>\n",
       "      <td>16:29:48</td>\n",
       "      <td>28424</td>\n",
       "      <td>59388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2015-01-07</td>\n",
       "      <td>07:53:28</td>\n",
       "      <td>16:30:56</td>\n",
       "      <td>28408</td>\n",
       "      <td>59456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2015-01-08</td>\n",
       "      <td>07:53:11</td>\n",
       "      <td>16:32:06</td>\n",
       "      <td>28391</td>\n",
       "      <td>59526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2015-01-09</td>\n",
       "      <td>07:52:50</td>\n",
       "      <td>16:33:18</td>\n",
       "      <td>28370</td>\n",
       "      <td>59598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2015-01-10</td>\n",
       "      <td>07:52:27</td>\n",
       "      <td>16:34:31</td>\n",
       "      <td>28347</td>\n",
       "      <td>59671</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Raw Data Analysis (RDA)\n",
    "\n",
    "Bevor wir Daten verarbeiten und Features bauen, prüfen wir die Rohdaten auf:\n",
    "- Struktur  (stündlich/täglich)\n",
    "- Vollständigkeit (Missing Values)\n",
    "- potenziell unnötige Spalten / redundante Informationen\n",
    "- Plausibilität und grobe Ausreißer\n",
    "\n",
    "Ziel dieser Analyse ist es, Entscheidungen für das spätere Data Processing abzuleiten,\n",
    "z.B. welche Spalten wir verwenden, welche wir entfernen und welche Daten wir zusammenführen.\n",
    "Ein Teil der Prüfung erfolgte auch manuell durch Sichtung der Tabellen (z.B. Auswahl von Österreich-Spalten bei Stromdaten).\n"
   ],
   "id": "57d72a1e83175460"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. Raw Data Analysis – Stromdaten (OPSD, Österreich)\n",
    "\n",
    "In den Stromdaten sind viele Länder enthalten. Für unser Projekt fokussieren wir auf Österreich und prüfen:\n",
    "- Vollständigkeit der relevanten AT-Spalten\n",
    "- Richtigkeit der Zeitstempel\n",
    "- Plausible Wertebereiche (keine negativen Last-/Erzeugungswerte)\n",
    "- erste statistische Kennzahlen und Korrelationen\n",
    "\n",
    "Diese Analyse dient als Grundlage für das spätere Data Processing \n"
   ],
   "id": "6d0fe00324632a5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-11T10:22:03.124949Z",
     "start_time": "2026-01-11T10:22:01.754715Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Analyse Stromdaten in AT\n",
    "\n",
    "# Pfad zur Datei\n",
    "file_path = \"data/raw/Strom/time_series_60min_singleindex.csv\"\n",
    "\n",
    "# CSV-Datei einlesen\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "print(\"Vorschau der Rohdaten:\")\n",
    "print(df.head(), \"\\n\")\n",
    "\n",
    "# Auswahl der relevanten Spalten (AT)\n",
    "# Nur österreichische Werte verwendet\n",
    "at_columns = [\n",
    "    \"utc_timestamp\",\n",
    "    \"cet_cest_timestamp\",\n",
    "    \"AT_load_actual_entsoe_transparency\",\n",
    "    \"AT_load_forecast_entsoe_transparency\",\n",
    "    \"AT_price_day_ahead\",\n",
    "    \"AT_solar_generation_actual\",\n",
    "    \"AT_wind_onshore_generation_actual\"\n",
    "]\n",
    "\n",
    "df_at = df[at_columns].copy()\n",
    "\n",
    "print(\"Vorschau der AT-Daten:\")\n",
    "print(df_at.head(), \"\\n\")\n",
    "\n",
    "# Vollständigkeit\n",
    "# Überprüfung auf fehlende Werte\n",
    "print(\"Fehlende Werte pro AT-Spalte:\")\n",
    "print(df_at.isnull().sum(), \"\\n\")\n",
    "\n",
    "# Datumsrichtigkeit\n",
    "# Umwandlung der Zeitstempel in Datumsformt\n",
    "df_at[\"utc_timestamp\"] = pd.to_datetime(\n",
    "    df_at[\"utc_timestamp\"], errors=\"coerce\"\n",
    ")\n",
    "\n",
    "df_at[\"cet_cest_timestamp\"] = pd.to_datetime(\n",
    "    df_at[\"cet_cest_timestamp\"], errors=\"coerce\"\n",
    ")\n",
    "\n",
    "# Überprüfung auf ungültige Zeitstempel\n",
    "print(\"Ungültige UTC-Zeitstempel:\")\n",
    "print(df_at[df_at[\"utc_timestamp\"].isnull()], \"\\n\")\n",
    "\n",
    "print(\"Ungültige CET/CEST-Zeitstempel:\")\n",
    "print(df_at[df_at[\"cet_cest_timestamp\"].isnull()], \"\\n\")\n",
    "\n",
    "# Wertebereiche\n",
    "# Überprüfung auf negative oder unrealistische Werte\n",
    "print(\"Negative Lastwerte (AT_load_actual):\")\n",
    "print(df_at[df_at[\"AT_load_actual_entsoe_transparency\"] < 0], \"\\n\")\n",
    "\n",
    "print(\"Negative Erzeugungswerte (Solar / Wind):\")\n",
    "print(\n",
    "    df_at[\n",
    "        (df_at[\"AT_solar_generation_actual\"] < 0) |\n",
    "        (df_at[\"AT_wind_onshore_generation_actual\"] < 0)\n",
    "    ],\n",
    "    \"\\n\"\n",
    ")\n",
    "\n",
    "# Statistische Analyse\n",
    "# Wichtigsten numerischen Spalten\n",
    "print(\"Deskriptive Statistik der AT-Stromdaten:\")\n",
    "print(\n",
    "    df_at[\n",
    "        [\n",
    "            \"AT_load_actual_entsoe_transparency\",\n",
    "            \"AT_load_forecast_entsoe_transparency\",\n",
    "            \"AT_price_day_ahead\",\n",
    "            \"AT_solar_generation_actual\",\n",
    "            \"AT_wind_onshore_generation_actual\"\n",
    "        ]\n",
    "    ].describe(),\n",
    "    \"\\n\"\n",
    ")\n",
    "\n",
    "# Analyse von Abweichungen\n",
    "# Differenz zwischen tatsächlicher und prognostizierter Last\n",
    "df_at[\"load_difference\"] = (\n",
    "    df_at[\"AT_load_actual_entsoe_transparency\"]\n",
    "    - df_at[\"AT_load_forecast_entsoe_transparency\"]\n",
    ")\n",
    "\n",
    "print(\"Abweichung zwischen Ist- und Prognoselast:\")\n",
    "print(df_at[\"load_difference\"].describe(), \"\\n\")\n",
    "\n",
    "\n",
    "# Korrelationen\n",
    "# Zusammenhängen zwischen Last, Preis und Erzeugung???\n",
    "correlation_matrix = df_at[\n",
    "    [\n",
    "        \"AT_load_actual_entsoe_transparency\",\n",
    "        \"AT_price_day_ahead\",\n",
    "        \"AT_solar_generation_actual\",\n",
    "        \"AT_wind_onshore_generation_actual\"\n",
    "    ]\n",
    "].corr()\n",
    "\n",
    "print(\"Korrelationsmatrix (AT-Daten):\")\n",
    "print(correlation_matrix, \"\\n\")\n"
   ],
   "id": "f25948b35d5943a2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vorschau der Rohdaten:\n",
      "          utc_timestamp        cet_cest_timestamp  \\\n",
      "0  2014-12-31T23:00:00Z  2015-01-01T00:00:00+0100   \n",
      "1  2015-01-01T00:00:00Z  2015-01-01T01:00:00+0100   \n",
      "2  2015-01-01T01:00:00Z  2015-01-01T02:00:00+0100   \n",
      "3  2015-01-01T02:00:00Z  2015-01-01T03:00:00+0100   \n",
      "4  2015-01-01T03:00:00Z  2015-01-01T04:00:00+0100   \n",
      "\n",
      "   AT_load_actual_entsoe_transparency  AT_load_forecast_entsoe_transparency  \\\n",
      "0                                 NaN                                   NaN   \n",
      "1                              5946.0                                6701.0   \n",
      "2                              5726.0                                6593.0   \n",
      "3                              5347.0                                6482.0   \n",
      "4                              5249.0                                6454.0   \n",
      "\n",
      "   AT_price_day_ahead  AT_solar_generation_actual  \\\n",
      "0                 NaN                         NaN   \n",
      "1                35.0                         NaN   \n",
      "2                45.0                         NaN   \n",
      "3                41.0                         NaN   \n",
      "4                38.0                         NaN   \n",
      "\n",
      "   AT_wind_onshore_generation_actual  BE_load_actual_entsoe_transparency  \\\n",
      "0                                NaN                                 NaN   \n",
      "1                               69.0                              9484.0   \n",
      "2                               64.0                              9152.0   \n",
      "3                               65.0                              8799.0   \n",
      "4                               64.0                              8567.0   \n",
      "\n",
      "   BE_load_forecast_entsoe_transparency  BE_solar_generation_actual  ...  \\\n",
      "0                                   NaN                         NaN  ...   \n",
      "1                                9897.0                         NaN  ...   \n",
      "2                                9521.0                         NaN  ...   \n",
      "3                                9135.0                         NaN  ...   \n",
      "4                                8909.0                         NaN  ...   \n",
      "\n",
      "   SI_load_actual_entsoe_transparency  SI_load_forecast_entsoe_transparency  \\\n",
      "0                                 NaN                                   NaN   \n",
      "1                                 NaN                                   NaN   \n",
      "2                             1045.47                                 816.0   \n",
      "3                             1004.79                                 805.0   \n",
      "4                              983.79                                 803.0   \n",
      "\n",
      "   SI_solar_generation_actual  SI_wind_onshore_generation_actual  \\\n",
      "0                         NaN                                NaN   \n",
      "1                         NaN                                NaN   \n",
      "2                         NaN                               1.17   \n",
      "3                         NaN                               1.04   \n",
      "4                         NaN                               1.61   \n",
      "\n",
      "   SK_load_actual_entsoe_transparency  SK_load_forecast_entsoe_transparency  \\\n",
      "0                                 NaN                                   NaN   \n",
      "1                                 NaN                                   NaN   \n",
      "2                              2728.0                                2860.0   \n",
      "3                              2626.0                                2810.0   \n",
      "4                              2618.0                                2780.0   \n",
      "\n",
      "   SK_solar_generation_actual  SK_wind_onshore_generation_actual  \\\n",
      "0                         NaN                                NaN   \n",
      "1                         NaN                                NaN   \n",
      "2                         3.8                                NaN   \n",
      "3                         3.8                                NaN   \n",
      "4                         3.8                                NaN   \n",
      "\n",
      "   UA_load_actual_entsoe_transparency  UA_load_forecast_entsoe_transparency  \n",
      "0                                 NaN                                   NaN  \n",
      "1                                 NaN                                   NaN  \n",
      "2                                 NaN                                   NaN  \n",
      "3                                 NaN                                   NaN  \n",
      "4                                 NaN                                   NaN  \n",
      "\n",
      "[5 rows x 300 columns] \n",
      "\n",
      "Vorschau der AT-Daten:\n",
      "          utc_timestamp        cet_cest_timestamp  \\\n",
      "0  2014-12-31T23:00:00Z  2015-01-01T00:00:00+0100   \n",
      "1  2015-01-01T00:00:00Z  2015-01-01T01:00:00+0100   \n",
      "2  2015-01-01T01:00:00Z  2015-01-01T02:00:00+0100   \n",
      "3  2015-01-01T02:00:00Z  2015-01-01T03:00:00+0100   \n",
      "4  2015-01-01T03:00:00Z  2015-01-01T04:00:00+0100   \n",
      "\n",
      "   AT_load_actual_entsoe_transparency  AT_load_forecast_entsoe_transparency  \\\n",
      "0                                 NaN                                   NaN   \n",
      "1                              5946.0                                6701.0   \n",
      "2                              5726.0                                6593.0   \n",
      "3                              5347.0                                6482.0   \n",
      "4                              5249.0                                6454.0   \n",
      "\n",
      "   AT_price_day_ahead  AT_solar_generation_actual  \\\n",
      "0                 NaN                         NaN   \n",
      "1                35.0                         NaN   \n",
      "2                45.0                         NaN   \n",
      "3                41.0                         NaN   \n",
      "4                38.0                         NaN   \n",
      "\n",
      "   AT_wind_onshore_generation_actual  \n",
      "0                                NaN  \n",
      "1                               69.0  \n",
      "2                               64.0  \n",
      "3                               65.0  \n",
      "4                               64.0   \n",
      "\n",
      "Fehlende Werte pro AT-Spalte:\n",
      "utc_timestamp                               0\n",
      "cet_cest_timestamp                          0\n",
      "AT_load_actual_entsoe_transparency          1\n",
      "AT_load_forecast_entsoe_transparency        1\n",
      "AT_price_day_ahead                      17556\n",
      "AT_solar_generation_actual                 62\n",
      "AT_wind_onshore_generation_actual          49\n",
      "dtype: int64 \n",
      "\n",
      "Ungültige UTC-Zeitstempel:\n",
      "Empty DataFrame\n",
      "Columns: [utc_timestamp, cet_cest_timestamp, AT_load_actual_entsoe_transparency, AT_load_forecast_entsoe_transparency, AT_price_day_ahead, AT_solar_generation_actual, AT_wind_onshore_generation_actual]\n",
      "Index: [] \n",
      "\n",
      "Ungültige CET/CEST-Zeitstempel:\n",
      "Empty DataFrame\n",
      "Columns: [utc_timestamp, cet_cest_timestamp, AT_load_actual_entsoe_transparency, AT_load_forecast_entsoe_transparency, AT_price_day_ahead, AT_solar_generation_actual, AT_wind_onshore_generation_actual]\n",
      "Index: [] \n",
      "\n",
      "Negative Lastwerte (AT_load_actual):\n",
      "Empty DataFrame\n",
      "Columns: [utc_timestamp, cet_cest_timestamp, AT_load_actual_entsoe_transparency, AT_load_forecast_entsoe_transparency, AT_price_day_ahead, AT_solar_generation_actual, AT_wind_onshore_generation_actual]\n",
      "Index: [] \n",
      "\n",
      "Negative Erzeugungswerte (Solar / Wind):\n",
      "Empty DataFrame\n",
      "Columns: [utc_timestamp, cet_cest_timestamp, AT_load_actual_entsoe_transparency, AT_load_forecast_entsoe_transparency, AT_price_day_ahead, AT_solar_generation_actual, AT_wind_onshore_generation_actual]\n",
      "Index: [] \n",
      "\n",
      "Deskriptive Statistik der AT-Stromdaten:\n",
      "       AT_load_actual_entsoe_transparency  \\\n",
      "count                        50400.000000   \n",
      "mean                          7070.148631   \n",
      "std                           1400.263167   \n",
      "min                            664.000000   \n",
      "25%                           5941.000000   \n",
      "50%                           7032.000000   \n",
      "75%                           8079.000000   \n",
      "max                          10803.000000   \n",
      "\n",
      "       AT_load_forecast_entsoe_transparency  AT_price_day_ahead  \\\n",
      "count                          50400.000000        32845.000000   \n",
      "mean                            7039.848552           33.934663   \n",
      "std                             1355.382837           14.944863   \n",
      "min                             3739.000000          -76.000000   \n",
      "25%                             5941.000000           25.000000   \n",
      "50%                             7018.000000           33.000000   \n",
      "75%                             7986.000000           42.000000   \n",
      "max                            11608.000000          187.000000   \n",
      "\n",
      "       AT_solar_generation_actual  AT_wind_onshore_generation_actual  \n",
      "count                50339.000000                       50352.000000  \n",
      "mean                   135.280121                         729.653261  \n",
      "std                    198.955378                         692.002497  \n",
      "min                      0.000000                           0.000000  \n",
      "25%                      0.000000                         162.000000  \n",
      "50%                     18.000000                         493.000000  \n",
      "75%                    224.000000                        1141.000000  \n",
      "max                   1166.000000                        2969.000000   \n",
      "\n",
      "Abweichung zwischen Ist- und Prognoselast:\n",
      "count    50400.000000\n",
      "mean        30.300079\n",
      "std        480.791562\n",
      "min      -6468.000000\n",
      "25%       -227.000000\n",
      "50%         55.000000\n",
      "75%        311.000000\n",
      "max       2907.000000\n",
      "Name: load_difference, dtype: float64 \n",
      "\n",
      "Korrelationsmatrix (AT-Daten):\n",
      "                                    AT_load_actual_entsoe_transparency  \\\n",
      "AT_load_actual_entsoe_transparency                            1.000000   \n",
      "AT_price_day_ahead                                            0.526790   \n",
      "AT_solar_generation_actual                                    0.212500   \n",
      "AT_wind_onshore_generation_actual                             0.063921   \n",
      "\n",
      "                                    AT_price_day_ahead  \\\n",
      "AT_load_actual_entsoe_transparency            0.526790   \n",
      "AT_price_day_ahead                            1.000000   \n",
      "AT_solar_generation_actual                    0.032643   \n",
      "AT_wind_onshore_generation_actual            -0.118076   \n",
      "\n",
      "                                    AT_solar_generation_actual  \\\n",
      "AT_load_actual_entsoe_transparency                    0.212500   \n",
      "AT_price_day_ahead                                    0.032643   \n",
      "AT_solar_generation_actual                            1.000000   \n",
      "AT_wind_onshore_generation_actual                    -0.097518   \n",
      "\n",
      "                                    AT_wind_onshore_generation_actual  \n",
      "AT_load_actual_entsoe_transparency                           0.063921  \n",
      "AT_price_day_ahead                                          -0.118076  \n",
      "AT_solar_generation_actual                                  -0.097518  \n",
      "AT_wind_onshore_generation_actual                            1.000000   \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dominik\\AppData\\Local\\Temp\\ipykernel_5588\\2098089143.py:42: FutureWarning: In a future version of pandas, parsing datetimes with mixed time zones will raise an error unless `utc=True`. Please specify `utc=True` to opt in to the new behaviour and silence this warning. To create a `Series` with mixed offsets and `object` dtype, please use `apply` and `datetime.datetime.strptime`\n",
      "  df_at[\"cet_cest_timestamp\"] = pd.to_datetime(\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Ergebnis: Die AT-Stromdaten sind bei utc_timestamp sowie Load/Solar/Wind nahezu vollständig (nur sehr wenige Missing), während AT_price_day_ahead viele fehlende Werte hat, weshalb wir im weiteren Processing auf utc_timestamp, load_mw, solar_mw und wind_mw fokussieren und den Preis weglassen.",
   "id": "12157662dfa54a68"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2.  Raw Data Analysis – Wetterdaten (Meteostat, stündlich)\n",
    "\n",
    "Die Wetterdaten liegen als Monats-CSV pro Stadt (Ordnerstruktur) vor.\n",
    "Für die Analyse werden alle Dateien eingelesen, zusammengeführt und auf Datenqualität geprüft:\n",
    "- Vollständigkeit je Feature\n",
    "- gültige Zeitstempel und Duplikate pro Stadt\n",
    "- Plausible Wertebereiche (Temp, Feuchte, Niederschlag, Wind, Druck)\n",
    "- einfache Statistiken und Korrelationen\n",
    "\n",
    "Die Ergebnisse dieser Analyse werden später im Processing genutzt (Feature-Auswahl, Cleaning, Merge-Strategie).\n"
   ],
   "id": "a047126f98090287"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-11T10:25:33.160871Z",
     "start_time": "2026-01-11T10:25:29.263793Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Wetterdaten-Analyse Österreich\n",
    "\n",
    "base_path = Path(\"data/raw/Wetterdata\")\n",
    "\n",
    "all_data = []\n",
    "\n",
    "for city_dir in base_path.iterdir():\n",
    "    if city_dir.is_dir():\n",
    "        city = city_dir.name  # Stadtname aus Ordner weil unterschiedlich geschrieben\n",
    "\n",
    "        for file in city_dir.glob(\"*.csv\"):\n",
    "            try:\n",
    "                # Dateiname\n",
    "                filename_parts = file.stem.split(\"_\")\n",
    "                year = int(filename_parts[-2])\n",
    "                month = int(filename_parts[-1])\n",
    "\n",
    "                # CSV-Datei einlesen\n",
    "                df = pd.read_csv(file)\n",
    "\n",
    "                # Zusatzinformationen ergänzen\n",
    "                df[\"city\"] = city\n",
    "                df[\"year\"] = year\n",
    "                df[\"month\"] = month\n",
    "\n",
    "                # Zeitstempel konvertieren\n",
    "                df[\"time\"] = pd.to_datetime(df[\"time\"], errors=\"coerce\")\n",
    "\n",
    "                all_data.append(df)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Fehler beim Einlesen von {file.name}: {e}\")\n",
    "\n",
    "# Alle Wetterdaten zusammenführen\n",
    "weather_df = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "print(\"Vorschau der zusammengeführten Wetterdaten:\")\n",
    "print(weather_df.head(), \"\\n\")\n",
    "\n",
    "# Vollständigkeit\n",
    "print(\"Fehlende Werte pro Spalte:\")\n",
    "print(weather_df.isnull().sum(), \"\\n\")\n",
    "\n",
    "print(\"Anzahl Datensätze insgesamt:\", len(weather_df))\n",
    "print(\"Anzahl Städte:\", weather_df[\"city\"].nunique(), \"\\n\")\n",
    "\n",
    "# Zeitstempel\n",
    "print(\"Ungültige Zeitstempel:\")\n",
    "print(weather_df[weather_df[\"time\"].isnull()], \"\\n\")\n",
    "\n",
    "# Doppelte Zeitstempel pro Stadt prüfen\n",
    "duplicates = weather_df.duplicated(subset=[\"city\", \"time\"])\n",
    "print(\"Anzahl doppelter Zeitstempel:\", duplicates.sum(), \"\\n\")\n",
    "\n",
    "# Realistische Werte\n",
    "# Realistische Wertebereiche\n",
    "invalid_temp = weather_df[(weather_df[\"temp\"] < -30) | (weather_df[\"temp\"] > 45)]\n",
    "invalid_rhum = weather_df[(weather_df[\"rhum\"] < 0) | (weather_df[\"rhum\"] > 100)]\n",
    "invalid_prcp = weather_df[weather_df[\"prcp\"] < 0]\n",
    "invalid_wind = weather_df[weather_df[\"wspd\"] < 0]\n",
    "invalid_pressure = weather_df[\n",
    "    (weather_df[\"pres\"].notnull()) &\n",
    "    ((weather_df[\"pres\"] < 850) | (weather_df[\"pres\"] > 1100))\n",
    "]\n",
    "\n",
    "print(\"Unplausible Temperaturwerte:\", len(invalid_temp))\n",
    "print(\"Unplausible Luftfeuchtewerte:\", len(invalid_rhum))\n",
    "print(\"Negative Niederschlagswerte:\", len(invalid_prcp))\n",
    "print(\"Negative Windgeschwindigkeiten:\", len(invalid_wind))\n",
    "print(\"Unplausible Luftdruckwerte:\", len(invalid_pressure), \"\\n\")\n",
    "\n",
    "# Statistische Analyse\n",
    "print(\"Deskriptive Statistik (Temperatur, Feuchte, Wind, Luftdruck):\")\n",
    "print(\n",
    "    weather_df[[\"temp\", \"rhum\", \"wspd\", \"pres\"]].describe(),\n",
    "    \"\\n\"\n",
    ")\n",
    "\n",
    "# Durchschnittstemperatur pro Stadt\n",
    "avg_temp_city = weather_df.groupby(\"city\")[\"temp\"].mean().sort_values()\n",
    "print(\"Durchschnittstemperatur pro Stadt:\")\n",
    "print(avg_temp_city, \"\\n\")\n",
    "\n",
    "# Korrelationen\n",
    "correlation_matrix = weather_df[\n",
    "    [\"temp\", \"rhum\", \"wspd\", \"pres\"]\n",
    "].corr()\n",
    "\n",
    "print(\"Korrelationsmatrix:\")\n",
    "print(correlation_matrix, \"\\n\")\n"
   ],
   "id": "192155901ed37f2a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vorschau der zusammengeführten Wetterdaten:\n",
      "                 time  temp  dwpt  rhum  prcp  snow  wdir  wspd  wpgt    pres  \\\n",
      "0 2015-01-01 00:00:00  -6.6  -7.0  97.0   0.0   NaN  30.0  14.0   NaN     NaN   \n",
      "1 2015-01-01 01:00:00  -6.9  -7.3  97.0   0.0   NaN  30.0  13.3   NaN     NaN   \n",
      "2 2015-01-01 02:00:00  -8.9  -9.7  94.0   0.0   NaN  30.0  10.1   NaN     NaN   \n",
      "3 2015-01-01 03:00:00  -9.4 -10.2  94.0   0.0   NaN   0.0   8.6   NaN  1038.8   \n",
      "4 2015-01-01 04:00:00  -9.0  -9.7  95.0   0.0   NaN  50.0  11.2   NaN     NaN   \n",
      "\n",
      "   tsun  coco     city  year  month  \n",
      "0   NaN   NaN  bregenz  2015      1  \n",
      "1   NaN   NaN  bregenz  2015      1  \n",
      "2   NaN   NaN  bregenz  2015      1  \n",
      "3   0.0   NaN  bregenz  2015      1  \n",
      "4   0.0   NaN  bregenz  2015      1   \n",
      "\n",
      "Fehlende Werte pro Spalte:\n",
      "time          0\n",
      "temp        180\n",
      "dwpt        187\n",
      "rhum        187\n",
      "prcp     131444\n",
      "snow     383012\n",
      "wdir      18013\n",
      "wspd        508\n",
      "wpgt     274031\n",
      "pres      38502\n",
      "tsun     232293\n",
      "coco     211153\n",
      "city          0\n",
      "year          0\n",
      "month         0\n",
      "dtype: int64 \n",
      "\n",
      "Anzahl Datensätze insgesamt: 394416\n",
      "Anzahl Städte: 9 \n",
      "\n",
      "Ungültige Zeitstempel:\n",
      "Empty DataFrame\n",
      "Columns: [time, temp, dwpt, rhum, prcp, snow, wdir, wspd, wpgt, pres, tsun, coco, city, year, month]\n",
      "Index: [] \n",
      "\n",
      "Anzahl doppelter Zeitstempel: 0 \n",
      "\n",
      "Unplausible Temperaturwerte: 0\n",
      "Unplausible Luftfeuchtewerte: 0\n",
      "Negative Niederschlagswerte: 0\n",
      "Negative Windgeschwindigkeiten: 0\n",
      "Unplausible Luftdruckwerte: 0 \n",
      "\n",
      "Deskriptive Statistik (Temperatur, Feuchte, Wind, Luftdruck):\n",
      "                temp           rhum           wspd           pres\n",
      "count  394236.000000  394229.000000  393908.000000  355914.000000\n",
      "mean       10.452450      75.415888       9.476339    1017.632915\n",
      "std         8.912208      18.334675       7.458594       8.355596\n",
      "min       -20.700000      12.000000       0.000000     973.600000\n",
      "25%         3.400000      62.000000       3.600000    1012.700000\n",
      "50%        10.100000      79.000000       7.200000    1017.400000\n",
      "75%        17.000000      91.000000      13.000000    1022.500000\n",
      "max        37.400000     100.000000      82.800000    1047.200000 \n",
      "\n",
      "Durchschnittstemperatur pro Stadt:\n",
      "city\n",
      "innsbruck      7.599961\n",
      "klagenfurt    10.070423\n",
      "bregenz       10.086850\n",
      "Salzburg      10.352788\n",
      "graz          10.495991\n",
      "Linz          10.647899\n",
      "st_poelten    11.226387\n",
      "eisenstadt    11.736032\n",
      "wien          11.858744\n",
      "Name: temp, dtype: float64 \n",
      "\n",
      "Korrelationsmatrix:\n",
      "          temp      rhum      wspd      pres\n",
      "temp  1.000000 -0.564226  0.073829 -0.257863\n",
      "rhum -0.564226  1.000000 -0.244908  0.105261\n",
      "wspd  0.073829 -0.244908  1.000000 -0.106122\n",
      "pres -0.257863  0.105261 -0.106122  1.000000 \n",
      "\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Ergebnis: Die Wetterdaten sind insgesamt gut nutzbar, jedoch sind mehrere Spalten stark unvollständig (u.a. snow, wpgt, tsun, coco und teilweise prcp), weshalb wir uns im weiteren Processing auf die weitgehend vollständigen Kernfeatures wie temp, rhum, wspd (und optional pres) konzentrieren.",
   "id": "b6a311731d1628b9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4. Raw Data Analysis – Feiertage (AT, Webscraping)\n",
    "\n",
    "Die Feiertage wurden per Webscraping gesammelt. In dieser Analyse prüfen wir grob:\n",
    "- Vollständigkeit der Rohdaten\n",
    "- ob das Datum korrekt extrahiert werden kann\n",
    "- Plausibilität des Wochentags (angegeben vs. aus Datum berechnet)\n",
    "\n",
    "Diese Daten werden später im Processing zu stündlichen Kalender-Features (z.B. `is_holiday`) umgewandelt.\n"
   ],
   "id": "391b0554fc2d1526"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-11T10:37:49.316718Z",
     "start_time": "2026-01-11T10:37:49.304211Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Analyse von Feiertagen und Wochentagen\n",
    "# ist aber Webscraping also sollte alles passen\n",
    "# und ich muss sagen ich hab bisschen was unnötiges da drinnen auch gemacht haha\n",
    "\n",
    "# Pfad\n",
    "file_path = \"data/raw/Feiertage/feiertage_at_2015_2020_raw.csv\"\n",
    "\n",
    "# CSV-Datei einlesen\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "print(\"Vorschau der Rohdaten:\")\n",
    "print(df.head(), \"\\n\")\n",
    "\n",
    "# Vollständigkeit\n",
    "# fehlende Werte pro Spalte\n",
    "print(\"Fehlende Werte pro Spalte:\")\n",
    "print(df.isnull().sum(), \"\\n\")\n",
    "\n",
    "# Trennung von Datum und Wochentag\n",
    "# Extraktion des Datums in TT.MM.YYYY\n",
    "df[\"date_str\"] = df[\"date_raw\"].str.extract(r\"(\\d{2}\\.\\d{2}\\.\\d{4})\")\n",
    "\n",
    "df[\"weekday_text\"] = df[\"date_raw\"].str.extract(r\"\\((.*?)\\)\")\n",
    "\n",
    "# Umwandlung in ein echtes Datumsformat\n",
    "df[\"date\"] = pd.to_datetime(\n",
    "    df[\"date_str\"],\n",
    "    format=\"%d.%m.%Y\",\n",
    "    errors=\"coerce\"\n",
    ")\n",
    "\n",
    "# Ausgabe von Zeilen mit ungültigen Wert\n",
    "print(\"Zeilen mit ungültigen Datumswerten:\")\n",
    "print(df[df[\"date\"].isnull()], \"\\n\")\n",
    "\n",
    "# Berechnung des Wochentags aus dem Datum (schauen ob das stimmt)\n",
    "# Mapping englischen auf deutsche\n",
    "weekday_map = {\n",
    "    \"Monday\": \"Montag\",\n",
    "    \"Tuesday\": \"Dienstag\",\n",
    "    \"Wednesday\": \"Mittwoch\",\n",
    "    \"Thursday\": \"Donnerstag\",\n",
    "    \"Friday\": \"Freitag\",\n",
    "    \"Saturday\": \"Samstag\",\n",
    "    \"Sunday\": \"Sonntag\"\n",
    "}\n",
    "\n",
    "# Berechnung des Wochentags aus Datum\n",
    "df[\"weekday_calc\"] = df[\"date\"].dt.day_name().map(weekday_map)\n",
    "\n",
    "# Vergleich zwischen angegebenem und berechnetem Wochentag\n",
    "df[\"weekday_match\"] = df[\"weekday_text\"] == df[\"weekday_calc\"]\n",
    "\n",
    "print(\"Anzahl inkorrekter Wochentagsangaben:\")\n",
    "print((~df[\"weekday_match\"]).sum(), \"\\n\")\n",
    "\n",
    "# Statistische Analyse\n",
    "\n",
    "# Anzahl der Feiertage pro Jahr\n",
    "holidays_per_year = df.groupby(\"year\").size()\n",
    "print(\"Anzahl der Feiertage pro Jahr:\")\n",
    "print(holidays_per_year, \"\\n\")\n",
    "\n",
    "# Verteilung der Feiertage nach Wochentagen\n",
    "weekday_distribution = df[\"weekday_calc\"].value_counts()\n",
    "print(\"Verteilung der Feiertage nach Wochentagen:\")\n",
    "print(weekday_distribution, \"\\n\")\n",
    "\n",
    "# Häufigste Feiertage\n",
    "most_common_holidays = df[\"holiday_name\"].value_counts().head(10)\n",
    "print(\"Häufigste Feiertage:\")\n",
    "print(most_common_holidays, \"\\n\")\n",
    "\n",
    "# Zusammenfassende Statistik\n",
    "\n",
    "print(\"Zusammenfassende Statistik:\")\n",
    "print(holidays_per_year.describe(), \"\\n\")\n"
   ],
   "id": "9b442463bc86f803",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vorschau der Rohdaten:\n",
      "   year         holiday_name                 date_raw\n",
      "0  2015              Neujahr  01.01.2015 (Donnerstag)\n",
      "1  2015  Heilige Drei Könige    06.01.2015 (Dienstag)\n",
      "2  2015          Ostermontag      06.04.2015 (Montag)\n",
      "3  2015       Staatsfeiertag     01.05.2015 (Freitag)\n",
      "4  2015  Christi Himmelfahrt  14.05.2015 (Donnerstag) \n",
      "\n",
      "Fehlende Werte pro Spalte:\n",
      "year            0\n",
      "holiday_name    0\n",
      "date_raw        0\n",
      "dtype: int64 \n",
      "\n",
      "Zeilen mit ungültigen Datumswerten:\n",
      "Empty DataFrame\n",
      "Columns: [year, holiday_name, date_raw, date_str, weekday_text, date]\n",
      "Index: [] \n",
      "\n",
      "Anzahl inkorrekter Wochentagsangaben:\n",
      "0 \n",
      "\n",
      "Anzahl der Feiertage pro Jahr:\n",
      "year\n",
      "2015    12\n",
      "2016    12\n",
      "2017    12\n",
      "2018    15\n",
      "2019    15\n",
      "2020    15\n",
      "dtype: int64 \n",
      "\n",
      "Verteilung der Feiertage nach Wochentagen:\n",
      "weekday_calc\n",
      "Montag        19\n",
      "Donnerstag    18\n",
      "Sonntag       12\n",
      "Dienstag       9\n",
      "Freitag        8\n",
      "Mittwoch       8\n",
      "Samstag        7\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "Häufigste Feiertage:\n",
      "holiday_name\n",
      "Neujahr                6\n",
      "Heilige Drei Könige    6\n",
      "Ostermontag            6\n",
      "Staatsfeiertag         6\n",
      "Christi Himmelfahrt    6\n",
      "Pfingstmontag          6\n",
      "Fronleichnam           6\n",
      "Mariä Himmelfahrt      6\n",
      "Nationalfeiertag       6\n",
      "Allerheiligen          6\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "Zusammenfassende Statistik:\n",
      "count     6.000000\n",
      "mean     13.500000\n",
      "std       1.643168\n",
      "min      12.000000\n",
      "25%      12.000000\n",
      "50%      13.500000\n",
      "75%      15.000000\n",
      "max      15.000000\n",
      "dtype: float64 \n",
      "\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Ergebnis: Die Feiertagsdaten sind vollständig und konsistent (keine fehlenden Werte, keine ungültigen Datumswerte und 0 falsche Wochentage), daher können wir sie direkt als zuverlässige Basis für das spätere is_holiday-Feature verwenden.",
   "id": "25409aa9efb27197"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4. Raw Data Analysis – Tageslicht (Wien, monatlich)\n",
    "\n",
    "Als erste Orientierung wurde zu Beginn eine monatliche Tageslänge für Wien analysiert.\n",
    "Damit prüfen wir Vollständigkeit, Format/Plausibilität und bekommen ein Gefühl für die saisonale Abhängigkeit.\n",
    "\n",
    "> Hinweis: Später wurde das Feature durch eine genauere tägliche Berechnung (Sunrise/Sunset, Österreich-Durchschnitt) ersetzt.\n"
   ],
   "id": "6be1942c6272592"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-11T10:19:11.797901Z",
     "start_time": "2026-01-11T10:19:11.785807Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Analyse der monatlichen Sonnenlänge in Wien\n",
    "\n",
    "file_path = \"data/raw/Tageslicht/sonnenlaenge_wien_monatlich.csv\"\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "print(\"Vorschau der Rohdaten:\")\n",
    "print(df, \"\\n\")\n",
    "\n",
    "# Vollständigkeit\n",
    "print(\"Fehlende Werte pro Spalte:\")\n",
    "print(df.isnull().sum(), \"\\n\")\n",
    "\n",
    "# Überprüfung, ob alle 12 Monate vorhanden sind\n",
    "print(\"Anzahl der Monate im Datensatz:\")\n",
    "print(len(df), \"\\n\")\n",
    "\n",
    "# Richtigkeit des Zeitformats\n",
    "# Auftrennen der Tageslänge in hh:mm\n",
    "time_split = df[\"day_length\"].str.split(\":\", expand=True)\n",
    "\n",
    "df[\"hours\"] = pd.to_numeric(time_split[0], errors=\"coerce\")\n",
    "df[\"minutes\"] = pd.to_numeric(time_split[1], errors=\"coerce\")\n",
    "\n",
    "# Umrechnung der Tageslänge in Minuten\n",
    "df[\"day_length_minutes\"] = df[\"hours\"] * 60 + df[\"minutes\"]\n",
    "\n",
    "print(\"Ungültige Zeitangaben:\")\n",
    "print(df[df[\"day_length_minutes\"].isnull()], \"\\n\")\n",
    "\n",
    "# Plausibilität?? (0–1440 Minuten)\n",
    "print(\"Unplausible Tageslängen:\")\n",
    "print(df[(df[\"day_length_minutes\"] < 0) | (df[\"day_length_minutes\"] > 1440)], \"\\n\")\n",
    "\n",
    "# Statistische Analyse\n",
    "print(\"Deskriptive Statistik der Tageslänge (Minuten):\")\n",
    "print(df[\"day_length_minutes\"].describe(), \"\\n\")\n",
    "\n",
    "# Monat mit längstem und kürzestem Tag\n",
    "max_day = df.loc[df[\"day_length_minutes\"].idxmax()]\n",
    "min_day = df.loc[df[\"day_length_minutes\"].idxmin()]\n",
    "\n",
    "print(\"Monat mit längster Tageslänge:\")\n",
    "print(max_day[[\"month\", \"day_length\"]], \"\\n\")\n",
    "\n",
    "print(\"Monat mit kürzester Tageslänge:\")\n",
    "print(min_day[[\"month\", \"day_length\"]], \"\\n\")\n",
    "\n",
    "# Abhängigkeit\n",
    "\n",
    "df[\"month_index\"] = range(1, len(df) + 1)\n",
    "\n",
    "correlation = df[\"month_index\"].corr(df[\"day_length_minutes\"])\n",
    "print(\"Korrelation zwischen Monatsindex und Tageslänge:\")\n",
    "print(correlation, \"\\n\")\n"
   ],
   "id": "7545feff75faf109",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vorschau der Rohdaten:\n",
      "        month day_length\n",
      "0      Jänner       8:52\n",
      "1     Februar      10:19\n",
      "2        März      11:56\n",
      "3       April      13:44\n",
      "4         Mai      15:16\n",
      "5        Juni      16:07\n",
      "6        Juli      15:44\n",
      "7      August      14:23\n",
      "8   September      12:39\n",
      "9     Oktober      10:55\n",
      "10   November       9:18\n",
      "11   Dezember       8:26 \n",
      "\n",
      "Fehlende Werte pro Spalte:\n",
      "month         0\n",
      "day_length    0\n",
      "dtype: int64 \n",
      "\n",
      "Anzahl der Monate im Datensatz:\n",
      "12 \n",
      "\n",
      "Ungültige Zeitangaben:\n",
      "Empty DataFrame\n",
      "Columns: [month, day_length, hours, minutes, day_length_minutes]\n",
      "Index: [] \n",
      "\n",
      "Unplausible Tageslängen:\n",
      "Empty DataFrame\n",
      "Columns: [month, day_length, hours, minutes, day_length_minutes]\n",
      "Index: [] \n",
      "\n",
      "Deskriptive Statistik der Tageslänge (Minuten):\n",
      "count     12.000000\n",
      "mean     738.250000\n",
      "std      165.204075\n",
      "min      506.000000\n",
      "25%      603.750000\n",
      "50%      737.500000\n",
      "75%      876.250000\n",
      "max      967.000000\n",
      "Name: day_length_minutes, dtype: float64 \n",
      "\n",
      "Monat mit längster Tageslänge:\n",
      "month          Juni\n",
      "day_length    16:07\n",
      "Name: 5, dtype: object \n",
      "\n",
      "Monat mit kürzester Tageslänge:\n",
      "month         Dezember\n",
      "day_length        8:26\n",
      "Name: 11, dtype: object \n",
      "\n",
      "Korrelation zwischen Monatsindex und Tageslänge:\n",
      "-0.1349934574328909 \n",
      "\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "567c0c4ee389c334"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Ergebnis: Wir sind draufgekommen dass die monatliche Tageslänge zu ungenau ist und haben danach die genauere tägliche Berechnung für ganz Österreich umgesetzt (diese jedoch nichtmehr in der data analysis gemacht.)",
   "id": "d2faa43acbcfb72a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Data Processing (Raw → Processed)\n",
    "\n",
    "Ziel der Verarbeitung ist es, alle Datenquellen auf ein **einheitliches Zeitformat (UTC, stündlich)** zu bringen,\n",
    "unnötige Spalten zu entfernen und die Datensätze so vorzubereiten, dass sie anschließend gemerged und auch in  InfluxDB gespeichert werden können.\n",
    "\n",
    "## 1. Stromdaten (OPSD) – Raw → Processed\n",
    "In diesem Schritt:\n",
    "- lesen wir die große OPSD-Zeitreihen-CSV ein,\n",
    "- filtern auf Österreich und die benötigten Features,\n",
    "- parsen den Zeitstempel als UTC,\n",
    "- filtern den Projektzeitraum (01/2015–12/2019),\n",
    "- benennen die Spalten auf ein einheitliches Schema um (`timestamp`, `load_mw`, `solar_mw`, `wind_mw`),\n",
    "- und speichern das Ergebnis als processed CSV.\n"
   ],
   "id": "1b31b67a0a0bade8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-11T11:07:10.900177Z",
     "start_time": "2026-01-11T11:07:09.833980Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# %%\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# %%\n",
    "# KONFIGURATION\n",
    "RAW_ENERGY_CSV = Path(\"data/raw/Strom/time_series_60min_singleindex.csv\")\n",
    "OUT_DIR = Path(\"data/processed/Strom\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(RAW_ENERGY_CSV.exists())\n",
    "OUT_FILE = OUT_DIR / \"energy_hourly_AT_2015_2019_processed.csv\"\n",
    "\n",
    "# Projekt-Zeitraum (UTC)\n",
    "START = \"2015-01-01 00:00:00\"\n",
    "END = \"2019-12-31 23:00:00\"\n",
    "\n",
    "# %%\n",
    "# CSV EINLESEN\n",
    "df = pd.read_csv(RAW_ENERGY_CSV)\n",
    "\n",
    "# %%\n",
    "# NUR DIE FEATURES BEHALTEN, DIE WIR BRAUCHEN\n",
    "keep_cols = [\n",
    "    \"utc_timestamp\",\n",
    "    \"AT_load_actual_entsoe_transparency\",\n",
    "    \"AT_solar_generation_actual\",\n",
    "    \"AT_wind_onshore_generation_actual\",\n",
    "    # \"AT_price_day_ahead\",\n",
    "    # \"AT_load_forecast_entsoe_transparency\",\n",
    "]\n",
    "\n",
    "# Prüfen, ob alle Spalten existieren (sonst Tippfehler / falsche Datei)\n",
    "missing = [c for c in keep_cols if c not in df.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Diese erwarteten Spalten fehlen in der CSV: {missing}\")\n",
    "\n",
    "df = df[keep_cols].copy()\n",
    "\n",
    "\n",
    "# %%\n",
    "# TIMESTAMP ALS UTC PARSEN\n",
    "# Beispiel-Format: 2015-01-01T00:00:00Z\n",
    "df[\"timestamp\"] = pd.to_datetime(df[\"utc_timestamp\"], utc=True, errors=\"coerce\")\n",
    "\n",
    "# Ungültige Zeitstempel entfernen\n",
    "df = df.dropna(subset=[\"timestamp\"]).copy()\n",
    "\n",
    "# OPTIONAL: Originalspalte entfernen (nur wenn du sie nicht mehr brauchst)\n",
    "df = df.drop(columns=[\"utc_timestamp\"])\n",
    "\n",
    "# %%\n",
    "# AUF PROJEKT-ZEITRAUM FILTERN\n",
    "start_ts = pd.Timestamp(START, tz=\"UTC\")\n",
    "end_ts = pd.Timestamp(END, tz=\"UTC\")\n",
    "\n",
    "df = df[(df[\"timestamp\"] >= start_ts) & (df[\"timestamp\"] <= end_ts)].copy()\n",
    "\n",
    "# Sortieren und doppelte Zeitstempel entfernen\n",
    "df = df.sort_values(\"timestamp\").drop_duplicates(subset=[\"timestamp\"], keep=\"first\")\n",
    "\n",
    "# %%\n",
    "# SPALTEN EINHEITLICH BENENNEN\n",
    "rename_map = {\n",
    "    \"utc_timestamp\": \"timestamp\",\n",
    "    \"AT_load_actual_entsoe_transparency\": \"load_mw\",\n",
    "    \"AT_solar_generation_actual\": \"solar_mw\",\n",
    "    \"AT_wind_onshore_generation_actual\": \"wind_mw\",\n",
    "    # optional:\n",
    "    # \"AT_price_day_ahead\": \"price_day_ahead\",\n",
    "    # \"AT_load_forecast_entsoe_transparency\": \"load_forecast_mw\",\n",
    "}\n",
    "df = df.rename(columns=rename_map)\n",
    "\n",
    "\n",
    "# Spalten-Reihenfolge festlegen\n",
    "ordered_cols = [\"timestamp\", \"load_mw\"]\n",
    "for c in [\"solar_mw\", \"wind_mw\", \"price_day_ahead\", \"load_forecast_mw\"]:\n",
    "    if c in df.columns:\n",
    "        ordered_cols.append(c)\n",
    "\n",
    "df = df[ordered_cols]\n",
    "\n",
    "# %%\n",
    "# ALS PROCESSED CSV SPEICHERN\n",
    "# Für DB-Import ist ein ISO-UTC-String oft am einfachsten (mit Z am Ende)\n",
    "df_out = df.copy()\n",
    "df_out[\"timestamp\"] = df_out[\"timestamp\"].dt.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "\n",
    "df_out.to_csv(OUT_FILE, index=False)\n",
    "print(\"Gespeichert:\", OUT_FILE.resolve())\n"
   ],
   "id": "808b85e522da4462",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Gespeichert: C:\\Users\\Dominik\\Desktop\\BigData-WeatherEnergy\\data\\processed\\Strom\\energy_hourly_AT_2015_2019_processed.csv\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. Wetterdaten (Meteostat) – Raw → Processed\n",
    "\n",
    "Die Wetterdaten liegen als viele einzelne Monatsdateien pro Stadt vor (2015-01 bis 2019-12).\n",
    "In diesem Schritt:\n",
    "- werden alle Monats-CSV pro Stadt zusammengeführt,\n",
    "- auf den Projektzeitraum gefiltert und auf stündliche Zeitstempel dedupliziert,\n",
    "- unnötige Spalten entfernt und ein einheitliches Schema erzeugt,\n",
    "- pro Stadt ein processed CSV gespeichert,\n",
    "- und anschließend ein Österreich-Durchschnitt (Mittelwert über alle Städte pro Stunde) berechnet.\n",
    "\n",
    "Ziel ist ein einheitliches stündliches Wetter-Feature-Set für den späteren Merge.\n"
   ],
   "id": "ce5c801bb96c0a26"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-11T11:10:58.530830Z",
     "start_time": "2026-01-11T11:10:54.869708Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# %%\n",
    "# KONFIGURATION\n",
    "RAW_WEATHER_DIR = Path(\"data/raw/Wetterdata\")\n",
    "OUT_DIR = Path(\"data/processed/Wetterdata/cities\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "CITIES = [\n",
    "    \"bregenz\",\n",
    "    \"eisenstadt\",\n",
    "    \"graz\",\n",
    "    \"innsbruck\",\n",
    "    \"klagenfurt\",\n",
    "    \"Linz\",\n",
    "    \"Salzburg\",\n",
    "    \"st_poelten\",\n",
    "    \"wien\",\n",
    "]\n",
    "\n",
    "# Projekt-Zeitraum (UTC)\n",
    "START = \"2015-01-01 00:00:00\"\n",
    "END = \"2019-12-31 23:00:00\"\n",
    "\n",
    "# Zu behaltende Spalten aus den Rohdaten\n",
    "KEEP_COLS_RAW = [\"time\", \"temp\", \"prcp\", \"rhum\", \"wspd\", \"pres\"]\n",
    "\n",
    "# Output-Dateiname pro Stadt\n",
    "def out_file_for_city(city: str) -> Path:\n",
    "    return OUT_DIR / f\"{city}_hourly_2015_2019.csv\"\n",
    "\n",
    "# %%\n",
    "# Funktion: alle Monatsdateien für eine Stadt finden\n",
    "def find_monthly_files(city: str) -> list[Path]:\n",
    "    city_dir = RAW_WEATHER_DIR / city\n",
    "    if not city_dir.exists():\n",
    "        raise FileNotFoundError(f\"Ordner nicht gefunden: {city_dir}\")\n",
    "\n",
    "\n",
    "    files = sorted(city_dir.glob(f\"{city}_*.csv\"))\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"Keine CSV-Dateien gefunden für {city} in {city_dir}\")\n",
    "\n",
    "    return files\n",
    "\n",
    "# %%\n",
    "# Hauptverarbeitung\n",
    "start_ts = pd.Timestamp(START, tz=\"UTC\")\n",
    "end_ts = pd.Timestamp(END, tz=\"UTC\")\n",
    "\n",
    "for city in CITIES:\n",
    "    print(f\"\\n--- Verarbeite Stadt: {city} ---\")\n",
    "\n",
    "    files = find_monthly_files(city)\n",
    "    print(f\"Gefundene Monatsfiles: {len(files)}\")\n",
    "\n",
    "    # Alle Monatsfiles einlesen & anhängen\n",
    "    dfs = []\n",
    "    for f in files:\n",
    "        df_m = pd.read_csv(f)\n",
    "\n",
    "        # Prüfen, ob alle benötigten Spalten existieren\n",
    "        missing = [c for c in KEEP_COLS_RAW if c not in df_m.columns]\n",
    "        if missing:\n",
    "            raise ValueError(f\"Datei {f.name} fehlt Spalten: {missing}\")\n",
    "\n",
    "        # Nur benötigte Spalten nehmen\n",
    "        df_m = df_m[KEEP_COLS_RAW].copy()\n",
    "        dfs.append(df_m)\n",
    "\n",
    "    df_city = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    # %%\n",
    "    # Zeit parsen\n",
    "    if \"timestamp\" not in df_city.columns:\n",
    "        if \"time\" not in df_city.columns:\n",
    "            raise KeyError(f\"Erwarte 'time' oder 'timestamp'. Vorhandene Spalten: {list(df_city.columns)}\")\n",
    "        df_city[\"timestamp\"] = pd.to_datetime(df_city[\"time\"], errors=\"coerce\", utc=True)\n",
    "\n",
    "    # Ungültige Zeiten entfernen\n",
    "    df_city = df_city.dropna(subset=[\"timestamp\"]).copy()\n",
    "\n",
    "    # Auf Projekt-Zeitraum filtern\n",
    "    df_city = df_city[(df_city[\"timestamp\"] >= start_ts) & (df_city[\"timestamp\"] <= end_ts)].copy()\n",
    "\n",
    "    # Doppelte Stunden entfernen\n",
    "    df_city = df_city.sort_values(\"timestamp\").drop_duplicates(subset=[\"timestamp\"], keep=\"first\")\n",
    "\n",
    "    # %%\n",
    "    # Spalten final umbenennen / aufräumen\n",
    "    if \"time\" in df_city.columns:\n",
    "        df_city = df_city.drop(columns=[\"time\"])\n",
    "\n",
    "    # Abkürzungen in verständlichere Namen umbenennen\n",
    "    rename_map = {\n",
    "        \"temp\": \"temp_c\",\n",
    "        \"prcp\": \"precip_mm\",\n",
    "        \"rhum\": \"humidity_pct\",\n",
    "        \"wspd\": \"wind_speed_kmh\",\n",
    "        \"pres\": \"pressure_hpa\",\n",
    "    }\n",
    "    df_city = df_city.rename(columns=rename_map)\n",
    "    # Stadtspalte mitgeben (für spätere Debugs / Merge)\n",
    "    df_city[\"city\"] = city\n",
    "\n",
    "    # Spaltenreihenfolge\n",
    "    df_city = df_city[[\"timestamp\", \"city\", \"temp_c\", \"precip_mm\", \"humidity_pct\", \"wind_speed_kmh\", \"pressure_hpa\"]]\n",
    "\n",
    "    # %%\n",
    "    # Timestamp in gleiches Format wie Strom bringen (ISO UTC mit Z)\n",
    "    df_out = df_city.copy()\n",
    "    df_out[\"timestamp\"] = df_out[\"timestamp\"].dt.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "\n",
    "    # Speichern\n",
    "    out_path = out_file_for_city(city)\n",
    "    df_out.to_csv(out_path, index=False)\n",
    "\n",
    "    print(f\"Gespeichert: {out_path} | Zeilen: {len(df_out)}\")\n",
    "    print(\"Von/Bis:\", df_city[\"timestamp\"].min(), \"->\", df_city[\"timestamp\"].max())\n",
    "\n",
    "print(\"\\nFertig.\")\n",
    "\n",
    "# %%\n",
    "# AT-AGGREGATION: Mittelwert über alle Städte pro Stunde\n",
    "# Ergebnis: eine Zeile pro Stunde für ganz Österreich\n",
    "\n",
    "\n",
    "# Alle processed City-Files einlesen\n",
    "city_files = [out_file_for_city(city) for city in CITIES]\n",
    "\n",
    "dfs = []\n",
    "for f in city_files:\n",
    "    if not f.exists():\n",
    "        raise FileNotFoundError(f\"Processed City-File fehlt: {f}\")\n",
    "    df_c = pd.read_csv(f)\n",
    "\n",
    "    # timestamp parsen (ist aktuell ISO-String mit Z)\n",
    "    df_c[\"timestamp\"] = pd.to_datetime(df_c[\"timestamp\"], utc=True, errors=\"coerce\")\n",
    "    df_c = df_c.dropna(subset=[\"timestamp\"]).copy()\n",
    "\n",
    "    dfs.append(df_c)\n",
    "\n",
    "df_all = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Mittelwert pro Stunde über alle Städte berechnen\n",
    "df_at = (\n",
    "    df_all\n",
    "    .groupby(\"timestamp\", as_index=False)[\n",
    "        [\"temp_c\", \"precip_mm\", \"humidity_pct\", \"wind_speed_kmh\", \"pressure_hpa\"]\n",
    "    ]\n",
    "    .mean()\n",
    ")\n",
    "\n",
    "\n",
    "# Spaltenreihenfolge\n",
    "df_at = df_at[[\"timestamp\", \"temp_c\", \"precip_mm\", \"humidity_pct\", \"wind_speed_kmh\", \"pressure_hpa\"]]\n",
    "\n",
    "# Timestamp wieder ins gleiche Format wie Strom bringen (ISO UTC mit Z)\n",
    "df_at[\"timestamp\"] = df_at[\"timestamp\"].dt.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "\n",
    "# Speichern\n",
    "AT_OUT_FILE = OUT_DIR.parent / \"weather_hourly_AT_2015_2019.csv\"  # data/processed/Wetterdata/weather_hourly_AT_2015_2019.csv\n",
    "df_at.to_csv(AT_OUT_FILE, index=False)\n",
    "\n",
    "print(\"AT-File gespeichert:\", AT_OUT_FILE.resolve(), \"| Zeilen:\", len(df_at))\n"
   ],
   "id": "9a090a938ab30842",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Verarbeite Stadt: bregenz ---\n",
      "Gefundene Monatsfiles: 60\n",
      "Gespeichert: data\\processed\\Wetterdata\\cities\\bregenz_hourly_2015_2019.csv | Zeilen: 43824\n",
      "Von/Bis: 2015-01-01 00:00:00+00:00 -> 2019-12-31 23:00:00+00:00\n",
      "\n",
      "--- Verarbeite Stadt: eisenstadt ---\n",
      "Gefundene Monatsfiles: 60\n",
      "Gespeichert: data\\processed\\Wetterdata\\cities\\eisenstadt_hourly_2015_2019.csv | Zeilen: 43824\n",
      "Von/Bis: 2015-01-01 00:00:00+00:00 -> 2019-12-31 23:00:00+00:00\n",
      "\n",
      "--- Verarbeite Stadt: graz ---\n",
      "Gefundene Monatsfiles: 60\n",
      "Gespeichert: data\\processed\\Wetterdata\\cities\\graz_hourly_2015_2019.csv | Zeilen: 43824\n",
      "Von/Bis: 2015-01-01 00:00:00+00:00 -> 2019-12-31 23:00:00+00:00\n",
      "\n",
      "--- Verarbeite Stadt: innsbruck ---\n",
      "Gefundene Monatsfiles: 60\n",
      "Gespeichert: data\\processed\\Wetterdata\\cities\\innsbruck_hourly_2015_2019.csv | Zeilen: 43824\n",
      "Von/Bis: 2015-01-01 00:00:00+00:00 -> 2019-12-31 23:00:00+00:00\n",
      "\n",
      "--- Verarbeite Stadt: klagenfurt ---\n",
      "Gefundene Monatsfiles: 60\n",
      "Gespeichert: data\\processed\\Wetterdata\\cities\\klagenfurt_hourly_2015_2019.csv | Zeilen: 43824\n",
      "Von/Bis: 2015-01-01 00:00:00+00:00 -> 2019-12-31 23:00:00+00:00\n",
      "\n",
      "--- Verarbeite Stadt: Linz ---\n",
      "Gefundene Monatsfiles: 60\n",
      "Gespeichert: data\\processed\\Wetterdata\\cities\\Linz_hourly_2015_2019.csv | Zeilen: 43824\n",
      "Von/Bis: 2015-01-01 00:00:00+00:00 -> 2019-12-31 23:00:00+00:00\n",
      "\n",
      "--- Verarbeite Stadt: Salzburg ---\n",
      "Gefundene Monatsfiles: 60\n",
      "Gespeichert: data\\processed\\Wetterdata\\cities\\Salzburg_hourly_2015_2019.csv | Zeilen: 43824\n",
      "Von/Bis: 2015-01-01 00:00:00+00:00 -> 2019-12-31 23:00:00+00:00\n",
      "\n",
      "--- Verarbeite Stadt: st_poelten ---\n",
      "Gefundene Monatsfiles: 60\n",
      "Gespeichert: data\\processed\\Wetterdata\\cities\\st_poelten_hourly_2015_2019.csv | Zeilen: 43824\n",
      "Von/Bis: 2015-01-01 00:00:00+00:00 -> 2019-12-31 23:00:00+00:00\n",
      "\n",
      "--- Verarbeite Stadt: wien ---\n",
      "Gefundene Monatsfiles: 60\n",
      "Gespeichert: data\\processed\\Wetterdata\\cities\\wien_hourly_2015_2019.csv | Zeilen: 43824\n",
      "Von/Bis: 2015-01-01 00:00:00+00:00 -> 2019-12-31 23:00:00+00:00\n",
      "\n",
      "Fertig.\n",
      "AT-File gespeichert: C:\\Users\\Dominik\\Desktop\\BigData-WeatherEnergy\\data\\processed\\Wetterdata\\weather_hourly_AT_2015_2019.csv | Zeilen: 43824\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. Tageslicht (AT) – Raw → Processed\n",
    "\n",
    "Die Tageslichtdaten liegen zunächst **täglich** vor (Sunrise/Sunset pro Tag).\n",
    "Für den späteren stündlichen Merge benötigen wir jedoch eine **stündliche Zeitreihe**.\n",
    "\n",
    "In diesem Schritt:\n",
    "- parsen wir das Datum und filtern den Projektzeitraum (2015–2019),\n",
    "- berechnen die Tageslänge (`daylight_seconds`, `daylight_hours`),\n",
    "- speichern eine tägliche Version,\n",
    "- und erzeugen daraus eine stündliche Tabelle (pro Tag 24 Stunden mit gleichem Tageslichtwert).\n"
   ],
   "id": "2df861b7552df64a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-11T11:13:09.917155Z",
     "start_time": "2026-01-11T11:13:09.709217Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# %%\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# %%\n",
    "# KONFIGURATION\n",
    "RAW_FILE = Path(\"data/raw/Tageslicht/austria_sunrise_sunset_avg_last5y_daily.csv\")\n",
    "OUT_DIR = Path(\"data/processed/Tageslicht\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "OUT_DAILY = OUT_DIR / \"daylight_daily_AT_2015_2019.csv\"\n",
    "OUT_HOURLY = OUT_DIR / \"daylight_hourly_AT_2015_2019.csv\"\n",
    "\n",
    "START_DATE = \"2015-01-01\"\n",
    "END_DATE = \"2019-12-31\"\n",
    "\n",
    "# %%\n",
    "# EINLESEN\n",
    "df = pd.read_csv(RAW_FILE)\n",
    "\n",
    "# %%\n",
    "# DATE PARSEN + FILTERN\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "df = df.dropna(subset=[\"date\"]).copy()\n",
    "\n",
    "df = df[(df[\"date\"] >= START_DATE) & (df[\"date\"] <= END_DATE)].copy()\n",
    "df = df.sort_values(\"date\")\n",
    "\n",
    "# %%\n",
    "# FEATURE ENGINEERING: Tageslänge\n",
    "\n",
    "df[\"daylight_seconds\"] = df[\"sunset_avg_seconds\"] - df[\"sunrise_avg_seconds\"]\n",
    "df[\"daylight_hours\"] = df[\"daylight_seconds\"] / 3600.0\n",
    "\n",
    "\n",
    "# Nur sinnvolle Spalten behalten\n",
    "df_daily = df[[\"date\", \"daylight_hours\", \"daylight_seconds\"]].copy()\n",
    "\n",
    "# Speichern (daily)\n",
    "df_daily.to_csv(OUT_DAILY, index=False)\n",
    "print(\"Gespeichert (daily):\", OUT_DAILY.resolve(), \"| Zeilen:\", len(df_daily))\n",
    "\n",
    "# %%\n",
    "# Wir erstellen pro Tag 24 Stunden-Timestamps und hängen daylight_hours dran.\n",
    "hours = pd.date_range(start=f\"{START_DATE} 00:00:00\", end=f\"{END_DATE} 23:00:00\", freq=\"H\", tz=\"UTC\")\n",
    "df_hourly = pd.DataFrame({\"timestamp\": hours})\n",
    "\n",
    "# Join: hourly timestamp -> date, dann mit df_daily matchen\n",
    "df_hourly[\"date\"] = df_hourly[\"timestamp\"].dt.floor(\"D\").dt.tz_localize(None)  # date ohne tz\n",
    "df_hourly = df_hourly.merge(df_daily, on=\"date\", how=\"left\")\n",
    "\n",
    "# timestamp formatieren wie bei euren anderen processed files\n",
    "df_hourly[\"timestamp\"] = df_hourly[\"timestamp\"].dt.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "\n",
    "# Aufräumen\n",
    "df_hourly = df_hourly.drop(columns=[\"date\"])\n",
    "df_hourly = df_hourly[[\"timestamp\", \"daylight_hours\", \"daylight_seconds\"]]\n",
    "\n",
    "# Speichern (hourly)\n",
    "df_hourly.to_csv(OUT_HOURLY, index=False)\n",
    "print(\"Gespeichert (hourly):\", OUT_HOURLY.resolve(), \"| Zeilen:\", len(df_hourly))\n"
   ],
   "id": "246df4d26d2e5506",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gespeichert (daily): C:\\Users\\Dominik\\Desktop\\BigData-WeatherEnergy\\data\\processed\\Tageslicht\\daylight_daily_AT_2015_2019.csv | Zeilen: 1826\n",
      "Gespeichert (hourly): C:\\Users\\Dominik\\Desktop\\BigData-WeatherEnergy\\data\\processed\\Tageslicht\\daylight_hourly_AT_2015_2019.csv | Zeilen: 43824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dominik\\AppData\\Local\\Temp\\ipykernel_5588\\7679528.py:45: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  hours = pd.date_range(start=f\"{START_DATE} 00:00:00\", end=f\"{END_DATE} 23:00:00\", freq=\"H\", tz=\"UTC\")\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 7.4 Kalender/Feiertage (AT) – Raw → Processed\n",
    "\n",
    "Die Feiertage liegen als Datumsliste (daily) vor, unser finales Feature-Dataset ist jedoch stündlich.\n",
    "Daher erzeugen wir eine stündliche Kalender-Zeitreihe (2015–2019) und berechnen darauf:\n",
    "\n",
    "- `weekday` (0=Montag … 6=Sonntag)\n",
    "- `is_weekend` (Samstag/Sonntag)\n",
    "- `is_holiday` (Datum ist in der Feiertagsliste enthalten)\n",
    "\n",
    "Das Ergebnis wird als processed CSV gespeichert und kann später direkt auf `timestamp` gejoint werden.\n"
   ],
   "id": "ee80a20476325964"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-11T11:15:21.951109Z",
     "start_time": "2026-01-11T11:15:21.761317Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# %%\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# %%\n",
    "# KONFIGURATION\n",
    "RAW_HOLIDAYS = Path(\"data/raw/Feiertage/feiertage_at_2015_2020_clean.csv\")\n",
    "OUT_DIR = Path(\"data/processed/Kalender\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "OUT_HOURLY = OUT_DIR / \"calendar_hourly_AT_2015_2019.csv\"\n",
    "\n",
    "START_DATE = \"2015-01-01\"\n",
    "END_DATE = \"2019-12-31\"\n",
    "\n",
    "# %%\n",
    "# FEIERTAGE EINLESEN\n",
    "df_h = pd.read_csv(RAW_HOLIDAYS)\n",
    "\n",
    "# date parsen\n",
    "df_h[\"date\"] = pd.to_datetime(df_h[\"date\"], errors=\"coerce\")\n",
    "df_h = df_h.dropna(subset=[\"date\"]).copy()\n",
    "\n",
    "# nur Projektzeitraum (daily)\n",
    "df_h = df_h[(df_h[\"date\"] >= START_DATE) & (df_h[\"date\"] <= END_DATE)].copy()\n",
    "\n",
    "# Set mit Feiertags-Daten (für schnellen Lookup)\n",
    "holiday_dates = set(df_h[\"date\"].dt.date)\n",
    "\n",
    "print(\"Feiertage im Zeitraum:\", len(holiday_dates))\n",
    "\n",
    "# %%\n",
    "# HOURLY ZEITACHSE ERZEUGEN (UTC)\n",
    "hours = pd.date_range(\n",
    "    start=f\"{START_DATE} 00:00:00\",\n",
    "    end=f\"{END_DATE} 23:00:00\",\n",
    "    freq=\"H\",\n",
    "    tz=\"UTC\"\n",
    ")\n",
    "df = pd.DataFrame({\"timestamp\": hours})\n",
    "\n",
    "# %%\n",
    "# FEATURES BERECHNEN\n",
    "# weekday: 0=Mon, ..., 6=Sun\n",
    "df[\"weekday\"] = df[\"timestamp\"].dt.weekday\n",
    "\n",
    "# is_weekend: Samstag(5) oder Sonntag(6)\n",
    "df[\"is_weekend\"] = df[\"weekday\"].isin([5, 6]).astype(int)\n",
    "\n",
    "# is_holiday: wenn das Datum in holiday_dates vorkommt\n",
    "df[\"date\"] = df[\"timestamp\"].dt.date\n",
    "df[\"is_holiday\"] = df[\"date\"].isin(holiday_dates).astype(int)\n",
    "\n",
    "\n",
    "\n",
    "# Aufräumen\n",
    "df = df.drop(columns=[\"date\"])\n",
    "\n",
    "# timestamp formatieren wie in euren anderen processed files\n",
    "df[\"timestamp\"] = df[\"timestamp\"].dt.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "\n",
    "# Spaltenreihenfolge\n",
    "df = df[[\"timestamp\", \"weekday\", \"is_weekend\", \"is_holiday\"]]\n",
    "\n",
    "# %%\n",
    "# SPEICHERN\n",
    "df.to_csv(OUT_HOURLY, index=False)\n",
    "print(\"Gespeichert:\", OUT_HOURLY.resolve(), \"| Zeilen:\", len(df))\n",
    "print(df.head(10).to_string(index=False))\n"
   ],
   "id": "1f923a60e9b1db15",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feiertage im Zeitraum: 66\n",
      "Gespeichert: C:\\Users\\Dominik\\Desktop\\BigData-WeatherEnergy\\data\\processed\\Kalender\\calendar_hourly_AT_2015_2019.csv | Zeilen: 43824\n",
      "           timestamp  weekday  is_weekend  is_holiday\n",
      "2015-01-01T00:00:00Z        3           0           1\n",
      "2015-01-01T01:00:00Z        3           0           1\n",
      "2015-01-01T02:00:00Z        3           0           1\n",
      "2015-01-01T03:00:00Z        3           0           1\n",
      "2015-01-01T04:00:00Z        3           0           1\n",
      "2015-01-01T05:00:00Z        3           0           1\n",
      "2015-01-01T06:00:00Z        3           0           1\n",
      "2015-01-01T07:00:00Z        3           0           1\n",
      "2015-01-01T08:00:00Z        3           0           1\n",
      "2015-01-01T09:00:00Z        3           0           1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dominik\\AppData\\Local\\Temp\\ipykernel_5588\\398862328.py:34: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  hours = pd.date_range(\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 5. Finaler Merge – Feature Table (stündlich, UTC)\n",
    "\n",
    "Nachdem alle Quellen im Processing auf ein einheitliches Format gebracht wurden (stündlich, UTC, konsistente Spaltennamen),\n",
    "führen wir sie zu einer gemeinsamen Feature-Tabelle zusammen.\n",
    "\n",
    "Dieses Ergebnis ist:\n",
    "- die zentrale Tabelle für das spätere Machine-Learning-Modell (SparkML),\n",
    "- und gleichzeitig die Datei, die in die Datenbank InfluxDB geladen wird.\n",
    "\n",
    "Merge-Strategie:\n",
    "- **Energy** ist die Basis (vollständige stündliche Zeitachse im Projektzeitraum)\n",
    "- Weather, Daylight und Calendar werden per **Left Join** auf `timestamp` ergänzt.\n"
   ],
   "id": "fa9f7cc2d32bd3c4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-11T11:17:55.078448Z",
     "start_time": "2026-01-11T11:17:54.419836Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# %%\n",
    "# Merge-Script: Energy + Weather + Daylight + Calendar  -> features_hourly_AT_2015_2019\n",
    "# Ziel:\n",
    "# - 4 processed CSVs einlesen\n",
    "# - timestamp überall als UTC datetime parsen\n",
    "# - auf timestamp mergen (stündlich)\n",
    "# - sauberes \"features\" CSV (und optional Parquet) schreiben\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# %%\n",
    "# KONFIGURATION\n",
    "ENERGY_FILE   = Path(\"data/processed/Strom/energy_hourly_AT_2015_2019_processed.csv\")\n",
    "WEATHER_FILE  = Path(\"data/processed/Wetterdata/weather_hourly_AT_2015_2019.csv\")\n",
    "DAYLIGHT_FILE = Path(\"data/processed/Tageslicht/daylight_hourly_AT_2015_2019.csv\")\n",
    "CAL_FILE      = Path(\"data/processed/Kalender/calendar_hourly_AT_2015_2019.csv\")\n",
    "\n",
    "OUT_DIR = Path(\"data/processed/features\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "OUT_CSV     = OUT_DIR / \"features_hourly_AT_2015_2019.csv\"\n",
    "OUT_PARQUET = OUT_DIR / \"features_hourly_AT_2015_2019.parquet\"  # für spark/ml\n",
    "\n",
    "# %%\n",
    "# CSV laden + timestamp parsen\n",
    "def load_with_timestamp(path: Path, ts_col: str = \"timestamp\") -> pd.DataFrame:\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Datei nicht gefunden: {path}\")\n",
    "\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    if ts_col not in df.columns:\n",
    "        raise KeyError(f\"Spalte '{ts_col}' fehlt in {path.name}. Spalten: {list(df.columns)}\")\n",
    "\n",
    "    # timestamp robust parsen (ISO mit Z / ohne Z / etc.)\n",
    "    df[ts_col] = pd.to_datetime(df[ts_col], utc=True, errors=\"coerce\")\n",
    "    df = df.dropna(subset=[ts_col]).copy()\n",
    "\n",
    "    # doppelte timestamps entfernen (sollte nicht vorkommen, aber sicher ist sicher)\n",
    "    df = df.sort_values(ts_col).drop_duplicates(subset=[ts_col], keep=\"first\")\n",
    "\n",
    "    return df\n",
    "\n",
    "# %%\n",
    "# DATEN LADEN\n",
    "df_energy   = load_with_timestamp(ENERGY_FILE, \"timestamp\")\n",
    "df_weather  = load_with_timestamp(WEATHER_FILE, \"timestamp\")\n",
    "df_daylight = load_with_timestamp(DAYLIGHT_FILE, \"timestamp\")\n",
    "df_cal      = load_with_timestamp(CAL_FILE, \"timestamp\")\n",
    "\n",
    "print(\"Energy:\", df_energy.shape, \"|\", df_energy.columns.tolist())\n",
    "print(\"Weather:\", df_weather.shape, \"|\", df_weather.columns.tolist())\n",
    "print(\"Daylight:\", df_daylight.shape, \"|\", df_daylight.columns.tolist())\n",
    "print(\"Calendar:\", df_cal.shape, \"|\", df_cal.columns.tolist())\n",
    "\n",
    "\n",
    "# %%\n",
    "# -----------------------------\n",
    "# MERGE (LEFT JOIN auf Energy als Basis)\n",
    "# -----------------------------\n",
    "df = df_energy.merge(df_weather, on=\"timestamp\", how=\"left\")\n",
    "df = df.merge(df_daylight, on=\"timestamp\", how=\"left\")\n",
    "df = df.merge(df_cal, on=\"timestamp\", how=\"left\")\n",
    "\n",
    "\n",
    "# %%\n",
    "# -----------------------------\n",
    "# QUICK CHECKS\n",
    "# -----------------------------\n",
    "print(\"\\nMerged shape:\", df.shape)\n",
    "print(\"Zeitraum:\", df[\"timestamp\"].min(), \"->\", df[\"timestamp\"].max())\n",
    "\n",
    "print(\"\\nFehlende Werte pro Spalte (Top 15):\")\n",
    "na_counts = df.isna().sum().sort_values(ascending=False)\n",
    "print(na_counts.head(15).to_string())\n",
    "\n",
    "print(\"\\nBeispiel (5 Zeilen):\")\n",
    "print(df.head(5).to_string(index=False))\n",
    "\n",
    "# %%\n",
    "# timestamp formatieren + speichern\n",
    "df_out = df.copy()\n",
    "df_out[\"timestamp\"] = df_out[\"timestamp\"].dt.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "\n",
    "df_out.to_csv(OUT_CSV, index=False)\n",
    "print(\"\\nGespeichert CSV:\", OUT_CSV.resolve(), \"| Zeilen:\", len(df_out))\n",
    "\n",
    "# Parquet für Spark/ML\n",
    "try:\n",
    "    df.to_parquet(OUT_PARQUET, index=False)\n",
    "    print(\"Gespeichert Parquet:\", OUT_PARQUET.resolve())\n",
    "except Exception as e:\n",
    "    print(\"Parquet konnte nicht gespeichert werden:\", e)\n"
   ],
   "id": "41e52f6f47cece2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Energy: (43824, 4) | ['timestamp', 'load_mw', 'solar_mw', 'wind_mw']\n",
      "Weather: (43824, 6) | ['timestamp', 'temp_c', 'precip_mm', 'humidity_pct', 'wind_speed_kmh', 'pressure_hpa']\n",
      "Daylight: (43824, 3) | ['timestamp', 'daylight_hours', 'daylight_seconds']\n",
      "Calendar: (43824, 4) | ['timestamp', 'weekday', 'is_weekend', 'is_holiday']\n",
      "\n",
      "Merged shape: (43824, 14)\n",
      "Zeitraum: 2015-01-01 00:00:00+00:00 -> 2019-12-31 23:00:00+00:00\n",
      "\n",
      "Fehlende Werte pro Spalte (Top 15):\n",
      "pressure_hpa        80\n",
      "solar_mw            55\n",
      "wind_mw             48\n",
      "load_mw              0\n",
      "temp_c               0\n",
      "timestamp            0\n",
      "precip_mm            0\n",
      "humidity_pct         0\n",
      "wind_speed_kmh       0\n",
      "daylight_hours       0\n",
      "daylight_seconds     0\n",
      "weekday              0\n",
      "is_weekend           0\n",
      "is_holiday           0\n",
      "\n",
      "Beispiel (5 Zeilen):\n",
      "                timestamp  load_mw  solar_mw  wind_mw    temp_c  precip_mm  humidity_pct  wind_speed_kmh  pressure_hpa  daylight_hours  daylight_seconds  weekday  is_weekend  is_holiday\n",
      "2015-01-01 00:00:00+00:00   5946.0       NaN     69.0 -5.155556      0.025     89.555556        6.555556   1037.885714        8.506389             30623        3           0           1\n",
      "2015-01-01 01:00:00+00:00   5726.0       NaN     64.0 -6.614286      0.000     88.285714        7.125000           NaN        8.506389             30623        3           0           1\n",
      "2015-01-01 02:00:00+00:00   5347.0       NaN     65.0 -6.075000      0.025     89.500000        6.712500           NaN        8.506389             30623        3           0           1\n",
      "2015-01-01 03:00:00+00:00   5249.0       NaN     64.0 -5.244444      0.000     88.222222        6.677778   1037.925000        8.506389             30623        3           0           1\n",
      "2015-01-01 04:00:00+00:00   5309.0       NaN     64.0 -5.525000      0.000     87.125000        7.612500           NaN        8.506389             30623        3           0           1\n",
      "\n",
      "Gespeichert CSV: C:\\Users\\Dominik\\Desktop\\BigData-WeatherEnergy\\data\\processed\\features\\features_hourly_AT_2015_2019.csv | Zeilen: 43824\n",
      "Gespeichert Parquet: C:\\Users\\Dominik\\Desktop\\BigData-WeatherEnergy\\data\\processed\\features\\features_hourly_AT_2015_2019.parquet\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Data Storage & Data Output Layer – InfluxDB + Grafana (NoSQL)\n",
    "\n",
    "Für den **Data Storage Layer** verwenden wir InfluxDB (NoSQL Time-Series DB).  \n",
    "Für den **Data Output Layer** nutzen wir Grafana zur Visualisierung und für eine Live-Demo.\n",
    "\n",
    "Da unser Projekt stündliche Zeitreihen verarbeitet, passt eine Time-Series DB sehr gut als Speicherform.\n",
    "Die Daten wurden nach dem Merge als finale Feature-Tabelle exportiert und anschließend in InfluxDB importiert.\n",
    "\n",
    "**Wichtig:** Viele Schritte (Setup, Import, Dashboard) wurden in der Weboberfläche durchgeführt – daher dokumentieren wir den Ablauf als HOW-TO.\n",
    "\n",
    "**Außerdem** Passwörter und Tokens wurden aus Sicherheitsgründen entfernt.\n"
   ],
   "id": "e2fe3f81822f2d32"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "services:\n",
    "  influxdb:\n",
    "    image: influxdb:2.7\n",
    "    container_name: influxdb\n",
    "    ports:\n",
    "      - \"8086:8086\"\n",
    "    volumes:\n",
    "      - influxdb-data:/var/lib/influxdb2\n",
    "    environment:\n",
    "      DOCKER_INFLUXDB_INIT_MODE: setup\n",
    "      DOCKER_INFLUXDB_INIT_USERNAME: admin\n",
    "      DOCKER_INFLUXDB_INIT_PASSWORD: \n",
    "      DOCKER_INFLUXDB_INIT_ORG: bigdata\n",
    "      DOCKER_INFLUXDB_INIT_BUCKET: energy_weather\n",
    "      DOCKER_INFLUXDB_INIT_ADMIN_TOKEN: \n",
    "    restart: unless-stopped\n",
    "\n",
    "  grafana:\n",
    "    image: grafana/grafana:latest\n",
    "    container_name: grafana\n",
    "    ports:\n",
    "      - \"3000:3000\"\n",
    "    volumes:\n",
    "      - grafana-data:/var/lib/grafana\n",
    "    depends_on:\n",
    "      - influxdb\n",
    "    restart: unless-stopped\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "volumes:\n",
    "  influxdb-data:\n",
    "  grafana-data:\n"
   ],
   "id": "81f953e645b3727d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Start der Infrastruktur (Docker Compose)\n",
    "\n",
    "1. `docker compose up -d`\n",
    "2. InfluxDB öffnen: `http://localhost:8086`\n",
    "3. Grafana öffnen: `http://localhost:3000`\n",
    "\n",
    "## 8.2 InfluxDB Setup (Web UI)\n",
    "\n",
    "InfluxDB wurde über die `DOCKER_INFLUXDB_INIT_*` Variablen automatisch initialisiert:\n",
    "- Org: `bigdata`\n",
    "- Bucket: `energy_weather`\n",
    "\n",
    "Anschließend wurde das finale CSV manuell importiert:\n",
    "- Datei: `data/processed/features/features_hourly_AT_2015_2019.csv`\n",
    "- timestamp-Spalte: `timestamp`\n",
    "- Measurements/Schema je nach Import-Wizard (Time Series Import)\n",
    "\n",
    "## 8.3 Grafana Setup (Web UI)\n",
    "\n",
    "1. Login Grafana (Default: `admin/admin`, beim ersten Login neues Passwort setzen)\n",
    "2. Datasource hinzufügen: **InfluxDB**\n",
    "   - URL: `http://influxdb:8086` (innerhalb Docker-Netz)\n",
    "   - Org/Bucket wie oben\n",
    "   - Token aus InfluxDB\n",
    "3. Dashboard erstellen (Beispiele):\n",
    "   - Load (`load_mw`) über Zeit\n",
    "   - Temperatur (`temp_c`) über Zeit\n",
    "   - Vergleich: Load vs Temperatur (2 Queries / 2 Achsen je nach Panel)\n",
    "   - Markierung von Wochenenden/Feiertagen (Filter oder Annotation möglich)\n",
    "\n",
    "**Ergebnis:** Visualisierung diente als Live-Demo und zur Plausibilitätsprüfung der Daten.\n"
   ],
   "id": "ce4c663407a3abb2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## SparkML hier",
   "id": "4d684a345ef03412"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
